/*
 * notorious_fft.h — Lil' FFT amalgamated single-header library
 *
 * Generated by amalgamate.py — DO NOT EDIT DIRECTLY.
 * Edit the module files in src/ and re-run CMake.
 *
 * Features:
 *   - Hardcoded kernels for N ≤ 64
 *   - Bluestein algorithm for arbitrary-size support
 *   - SIMD acceleration: NEON (AArch64/ARM), AVX2, AVX-512 (x86-64)
 *   - OpenMP parallelisation
 *   - minfft-compatible API (notorious_fft_ prefix)
 *
 * Usage (C):
 *   #define NOTORIOUS_FFT_IMPLEMENTATION
 *   #include "notorious_fft.h"
 *
 * Usage (C++):
 *   #define NOTORIOUS_FFT_IMPLEMENTATION
 *   #include "notorious_fft.h"   // or use the notorious_fft.hpp C++ wrapper
 *
 * Options (define before including):
 *   NOTORIOUS_FFT_SINGLE    — use float instead of double
 *   NOTORIOUS_FFT_FAST_MATH — enable fast Bhaskara I sin/cos approximation (~0.1% error)
 *   NOTORIOUS_FFT_OPENMP    — enable OpenMP (requires -fopenmp)
 *
 * License: MIT
 */

#ifndef NOTORIOUS_FFT_H
#define NOTORIOUS_FFT_H

#ifdef __cplusplus
extern "C" {
#endif


/* ==========================================================================
 * 01_core.h
 * ========================================================================== */

/*
 * Notorious FFT - Core Types and Platform Detection
 */


#include <stdlib.h>
#include <math.h>
#include <string.h>
#include <stddef.h>
#include <stdio.h>
#include <stdint.h>
#include <time.h>

/* ============================================================================
 * Configuration
 * ============================================================================ */

#ifndef NOTORIOUS_FFT_SMALL_SIZE
#define NOTORIOUS_FFT_SMALL_SIZE 8  /* Use iterative path for N ≤ this; N ≥ 16 uses split-radix DIF */
#endif

#ifndef NOTORIOUS_FFT_BLOCK_SIZE
#define NOTORIOUS_FFT_BLOCK_SIZE 4096  /* Cache block size for tiling */
#endif

#ifndef NOTORIOUS_FFT_TIMING_RUNS
#define NOTORIOUS_FFT_TIMING_RUNS 3  /* Runs for timing-based selection */
#endif

/* ============================================================================
 * Type Definitions
 * ============================================================================ */

#ifdef NOTORIOUS_FFT_SINGLE
typedef float notorious_fft_real;
#else
typedef double notorious_fft_real;
#endif

/* Split-complex: separate real and imaginary arrays */
typedef struct {
    notorious_fft_real *re;
    notorious_fft_real *im;
} notorious_fft_complex;

/* Forward declaration */
struct notorious_fft_plan;

/* Function pointer type for execute functions */
typedef void (*notorious_fft_execute_func_t)(const struct notorious_fft_plan*, 
                                       const notorious_fft_real*, const notorious_fft_real*,
                                       notorious_fft_real*, notorious_fft_real*);

/* FFT Plan - precomputed data for a specific size */
typedef struct notorious_fft_plan {
    size_t n;               /* Transform size (any positive integer) */

    notorious_fft_execute_func_t execute_func;

    /* Bump allocator: one slab holds the struct + all sub-allocations.
     * Suballocations decrement from the end — no per-free bookkeeping.
     * notorious_fft_destroy_plan frees just this one pointer. */
    void *slab;             /* Base of the single allocation */

    /* For power-of-2: bit-reversal and twiddles */
    int *bitrev;            /* Bit-reversal permutation table */
    notorious_fft_real *tw_re;     /* Twiddle factors (real part) */
    notorious_fft_real *tw_im;     /* Twiddle factors (imag part) */

    /* Working buffers (allocated at plan creation) */
    notorious_fft_real *work_re;   /* Working buffer (real) */
    notorious_fft_real *work_im;   /* Working buffer (imag) */

    /* Split-radix DIF twiddles (interleaved: e[4k], e[4k+1] = w1, e[4k+2], e[4k+3] = w3) */
    notorious_fft_real *sr_e;      /* Split-radix exponent table — same layout as minfft */
    notorious_fft_real *sr_t;      /* Temp buffer for split-radix (n complex = 2*n reals) */

    /* For Bluestein algorithm (non-power-of-2) */
    size_t bluestein_n;     /* Next power of 2 >= 2*n-1 */
    int is_inverse;         /* 1 for inverse FFT, 0 for forward */
    notorious_fft_real *bluestein_chirp_re;  /* Chirp factors exp(-i*pi*k^2/n) */
    notorious_fft_real *bluestein_chirp_im;
    notorious_fft_real *bluestein_chirp_fft_re;  /* FFT of chirp (conjugated) */
    notorious_fft_real *bluestein_chirp_fft_im;
    struct notorious_fft_plan *bluestein_plan;  /* Internal FFT plan (owns its own slab) */
    notorious_fft_real *bluestein_buf_re;  /* Convolution buffer for input/output */
    notorious_fft_real *bluestein_buf_im;
    notorious_fft_real *bluestein_fft_buf_re;  /* FFT output buffer */
    notorious_fft_real *bluestein_fft_buf_im;

} notorious_fft_plan;

/* Legacy aux structure for API compatibility */
typedef struct notorious_fft_aux {
    int N;
    void *t;
    void *e;
    struct notorious_fft_aux *sub1, *sub2;
    notorious_fft_plan *plan;
    /* Pre-allocated scratch buffers to avoid per-call malloc in DFT wrapper */
    notorious_fft_real *scratch_re;
    notorious_fft_real *scratch_im;
} notorious_fft_aux;

/* ============================================================================
 * Platform Detection
 * ============================================================================ */

/* MSVC compatibility */
#ifdef _MSC_VER
    #include <intrin.h>
    #define NOTORIOUS_FFT_INLINE __inline
    #define NOTORIOUS_FFT_ALIGN(x) __declspec(align(x))
    #define NOTORIOUS_FFT_RESTRICT __restrict
#else
    #define NOTORIOUS_FFT_INLINE inline __attribute__((always_inline))
    #define NOTORIOUS_FFT_ALIGN(x) __attribute__((aligned(x)))
    #ifdef __cplusplus
        #define NOTORIOUS_FFT_RESTRICT __restrict
    #else
        #define NOTORIOUS_FFT_RESTRICT restrict
    #endif
#endif

/* SIMD Detection */
#ifndef NOTORIOUS_FFT_HAS_NEON
    #if defined(__ARM_NEON) || defined(__ARM_NEON__)
        #include <arm_neon.h>
        #define NOTORIOUS_FFT_HAS_NEON 1
    #else
        #define NOTORIOUS_FFT_HAS_NEON 0
    #endif
#endif

#ifndef NOTORIOUS_FFT_HAS_AVX512
    #if defined(__AVX512F__) && defined(__AVX512DQ__)
        #include <immintrin.h>
        #define NOTORIOUS_FFT_HAS_AVX512 1
    #else
        #define NOTORIOUS_FFT_HAS_AVX512 0
    #endif
#endif

#ifndef NOTORIOUS_FFT_HAS_AVX2
    #if defined(__AVX2__)
        #include <immintrin.h>
        #define NOTORIOUS_FFT_HAS_AVX2 1
    #else
        #define NOTORIOUS_FFT_HAS_AVX2 0
    #endif
#endif

#ifndef NOTORIOUS_FFT_HAS_SSE2
    #if defined(__SSE2__) || (defined(_M_X64) || defined(_M_AMD64))
        #include <emmintrin.h>
        #define NOTORIOUS_FFT_HAS_SSE2 1
    #else
        #define NOTORIOUS_FFT_HAS_SSE2 0
    #endif
#endif

/* OpenMP Detection */
#ifdef _OPENMP
    #include <omp.h>
    #define NOTORIOUS_FFT_HAS_OPENMP 1
#else
    #define NOTORIOUS_FFT_HAS_OPENMP 0
#endif

/* High-resolution timer */
#if defined(_MSC_VER)
    static NOTORIOUS_FFT_INLINE uint64_t notorious_fft_rdtsc(void) {
        return __rdtsc();
    }
#elif defined(__x86_64__) || defined(__i386__)
    static NOTORIOUS_FFT_INLINE uint64_t notorious_fft_rdtsc(void) {
        unsigned int lo, hi;
        __asm__ __volatile__ ("rdtsc" : "=a" (lo), "=d" (hi));
        return ((uint64_t)hi << 32) | lo;
    }
#elif defined(__aarch64__)
    static NOTORIOUS_FFT_INLINE uint64_t notorious_fft_rdtsc(void) {
        uint64_t val;
        __asm__ __volatile__("mrs %0, cntvct_el0" : "=r" (val));
        return val;
    }
#else
    static NOTORIOUS_FFT_INLINE uint64_t notorious_fft_rdtsc(void) {
        struct timespec ts;
        clock_gettime(CLOCK_MONOTONIC, &ts);
        return (uint64_t)ts.tv_sec * 1000000000ULL + ts.tv_nsec;
    }
#endif

/* Constants */
static const notorious_fft_real NOTORIOUS_FFT_PI = 3.141592653589793238462643383279502884L;
static const notorious_fft_real NOTORIOUS_FFT_2PI = 6.283185307179586476925286766559005768L;
static const notorious_fft_real NOTORIOUS_FFT_INV_2PI = 0.159154943091895335768883763372514362L;
static const notorious_fft_real NOTORIOUS_FFT_SQRT2 = 1.414213562373095048801688724209698079L;
static const notorious_fft_real NOTORIOUS_FFT_INV_SQRT2 = 0.707106781186547524400844362104849039L;

/* ==========================================================================
 * 02_math.h
 * ========================================================================== */

/*
 * Notorious FFT - Math Utilities and Memory Management
 */


/* Include core types */

/* ============================================================================
 * Fast Math Approximations
 * ============================================================================ */

/* Bhaskara I sine approximation: sin(x) ≈ (16x(π-x))/(5π² - 4x(π-x))
 * Extended for full [-π, π] range using symmetries
 * Max error ~0.001 (0.1%) - sufficient for twiddle factors */
static NOTORIOUS_FFT_INLINE notorious_fft_real notorious_fft_sin_fast(notorious_fft_real x) {
    /* Reduce to [-π, π] */
    x = x - NOTORIOUS_FFT_2PI * floor(x * NOTORIOUS_FFT_INV_2PI + 0.5);
    
    /* Use symmetry: sin(-x) = -sin(x) */
    notorious_fft_real sign = 1.0;
    if (x < 0) {
        sign = -1.0;
        x = -x;
    }
    
    /* Reduce to [0, π] using sin(π - x) = sin(x) */
    if (x > NOTORIOUS_FFT_PI) {
        x = NOTORIOUS_FFT_2PI - x;
    }
    
    /* Reduce to [0, π/2] using sin(π - x) = sin(x) */
    if (x > NOTORIOUS_FFT_PI / 2) {
        x = NOTORIOUS_FFT_PI - x;
    }
    
    /* Bhaskara I approximation on [0, π/2] */
    notorious_fft_real x_by_pi = x / NOTORIOUS_FFT_PI;
    notorious_fft_real num = 16.0 * x_by_pi * (1.0 - x_by_pi);
    notorious_fft_real den = 5.0 - 4.0 * x_by_pi * (1.0 - x_by_pi);
    
    return sign * num / den;
}

static NOTORIOUS_FFT_INLINE notorious_fft_real notorious_fft_cos_fast(notorious_fft_real x) {
    return notorious_fft_sin_fast(x + NOTORIOUS_FFT_PI / 2.0);
}

/* Accurate versions using standard library */
static NOTORIOUS_FFT_INLINE notorious_fft_real notorious_fft_sin_accurate(notorious_fft_real x) {
    return (notorious_fft_real)sin((double)x);
}

static NOTORIOUS_FFT_INLINE notorious_fft_real notorious_fft_cos_accurate(notorious_fft_real x) {
    return (notorious_fft_real)cos((double)x);
}

/* Always use accurate sin/cos for twiddle factor precomputation.
 * The fast approximation (0.1% error) accumulates across FFT stages and
 * produces unacceptable numerical error.  NOTORIOUS_FFT_FAST_MATH enables it as
 * an opt-in for applications that can tolerate ~1% output error. */
#ifdef NOTORIOUS_FFT_FAST_MATH
    #define notorious_fft_sin notorious_fft_sin_fast
    #define notorious_fft_cos notorious_fft_cos_fast
#else
    #define notorious_fft_sin notorious_fft_sin_accurate
    #define notorious_fft_cos notorious_fft_cos_accurate
#endif

/* ============================================================================
 * Memory Management
 * ============================================================================ */

static NOTORIOUS_FFT_INLINE void* notorious_fft_malloc(size_t size) {
    void* ptr = NULL;
#if defined(_MSC_VER)
    ptr = _aligned_malloc(size, 64);
#elif defined(__APPLE__)
    if (posix_memalign(&ptr, 64, size) != 0) return NULL;
#else
    if (posix_memalign(&ptr, 64, size) != 0) return NULL;
#endif
    return ptr;
}

static NOTORIOUS_FFT_INLINE void notorious_fft_free(void* ptr) {
#if defined(_MSC_VER)
    _aligned_free(ptr);
#else
    free(ptr);
#endif
}

/* ============================================================================
 * Bump Allocator — decrement-from-end, no per-free bookkeeping
 *
 * Usage:
 *   1. Compute total bytes needed (sum of NOTORIOUS_FFT_BUMP_ALIGN-rounded sizes
 *      plus sizeof(notorious_fft_plan)).
 *   2. Call notorious_fft_malloc(total) to get the slab.
 *   3. Place notorious_fft_plan at slab[0..sizeof(notorious_fft_plan)-1].
 *   4. Set plan->bump = (char*)slab + total.
 *   5. Each notorious_fft_bump_alloc(plan, n) decrements plan->bump by the
 *      aligned size and returns the new pointer — no branch, no NULL check
 *      (the slab is pre-sized to fit all allocations exactly).
 *   6. notorious_fft_destroy_plan frees plan->slab in one call.
 *
 * All sub-allocations are 64-byte aligned because:
 *   - The slab base is 64-byte aligned (from notorious_fft_malloc).
 *   - Every allocation is rounded up to NOTORIOUS_FFT_BUMP_ALIGN (64) bytes.
 *   - Decrementing by a multiple of 64 from a 64-aligned end preserves
 *     alignment for every pointer.
 * ============================================================================ */

#define NOTORIOUS_FFT_BUMP_ALIGN 64u

/* Round n up to the next multiple of NOTORIOUS_FFT_BUMP_ALIGN */
#define NOTORIOUS_FFT_BUMP_ROUND(n) \
    (((size_t)(n) + NOTORIOUS_FFT_BUMP_ALIGN - 1u) & ~(size_t)(NOTORIOUS_FFT_BUMP_ALIGN - 1u))

/* Suballocate from the high end of the slab, decrementing the bump pointer.
 * 'bump' is a (char**) pointing to the current high-water mark. */
static NOTORIOUS_FFT_INLINE void* notorious_fft_bump_alloc(char** bump, size_t bytes) {
    *bump -= NOTORIOUS_FFT_BUMP_ROUND(bytes);
    return (void*)*bump;
}

/* ============================================================================
 * Bit Reversal
 * ============================================================================ */

static NOTORIOUS_FFT_INLINE void notorious_fft_compute_bitrev(int* rev, size_t n) {
    int log_n = 0;
    size_t temp = n;
    while (temp > 1) {
        temp >>= 1;
        log_n++;
    }
    
    for (size_t i = 0; i < n; i++) {
        size_t j = 0;
        for (int k = 0; k < log_n; k++) {
            if ((i >> k) & 1) {
                j |= (1 << (log_n - 1 - k));
            }
        }
        rev[i] = (int)j;
    }
}

/* ============================================================================
 * Butterfly Macros
 * ============================================================================ */

#define NOTORIOUS_FFT_BUTTERFLY(re, im, i, j, wr, wi) do { \
    notorious_fft_real ur = re[i], ui = im[i]; \
    notorious_fft_real vr = re[j] * (wr) - im[j] * (wi); \
    notorious_fft_real vi = re[j] * (wi) + im[j] * (wr); \
    re[i] = ur + vr; im[i] = ui + vi; \
    re[j] = ur - vr; im[j] = ui - vi; \
} while(0)

static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly_scalar(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT re, notorious_fft_real* NOTORIOUS_FFT_RESTRICT im,
    size_t i, size_t j, notorious_fft_real wr, notorious_fft_real wi
) {
    notorious_fft_real ur = re[i], ui = im[i];
    notorious_fft_real vr = re[j] * wr - im[j] * wi;
    notorious_fft_real vi = re[j] * wi + im[j] * wr;
    re[i] = ur + vr; im[i] = ui + vi;
    re[j] = ur - vr; im[j] = ui - vi;
}

/* ==========================================================================
 * 03_simd.h
 * ========================================================================== */

/*
 * Notorious FFT - SIMD Butterfly Operations
 */



/* ============================================================================
 * AVX-512 Implementation
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_AVX512

/* Process 8 elements (4 complex) at a time with gather */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly8_avx512(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT tw_re, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT tw_im,
    size_t i1, size_t i2, const int32_t* indices
) {
    __m512d ur = _mm512_loadu_pd(&wr[i1]);
    __m512d ui = _mm512_loadu_pd(&wi[i1]);
    __m512d vr = _mm512_loadu_pd(&wr[i2]);
    __m512d vi = _mm512_loadu_pd(&wi[i2]);
    
    /* Gather twiddles using 32-bit indices */
    __m256i idx = _mm256_loadu_si256((__m256i*)indices);
    __m512d t_wr = _mm512_i32gather_pd(idx, tw_re, 8);
    __m512d t_wi = _mm512_i32gather_pd(idx, tw_im, 8);
    
    /* Complex multiply: vr*wr - vi*wi + i*(vr*wi + vi*wr) */
    __m512d vro = _mm512_fmsub_pd(vr, t_wr, _mm512_mul_pd(vi, t_wi));
    __m512d vio = _mm512_fmadd_pd(vr, t_wi, _mm512_mul_pd(vi, t_wr));
    
    _mm512_storeu_pd(&wr[i1], _mm512_add_pd(ur, vro));
    _mm512_storeu_pd(&wi[i1], _mm512_add_pd(ui, vio));
    _mm512_storeu_pd(&wr[i2], _mm512_sub_pd(ur, vro));
    _mm512_storeu_pd(&wi[i2], _mm512_sub_pd(ui, vio));
}

/* Process single butterfly with AVX-512 (8 complex = 4 butterflies) - INVERSE */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly8_avx512_inverse(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    size_t i1, size_t i2,
    __m512d wr_vec, __m512d wi_vec
) {
    __m512d ur = _mm512_loadu_pd(&wr[i1]);
    __m512d ui = _mm512_loadu_pd(&wi[i1]);
    __m512d vr = _mm512_loadu_pd(&wr[i2]);
    __m512d vi = _mm512_loadu_pd(&wi[i2]);
    
    /* For inverse: use conj(w) = (wr, -wi), so vr*wr + vi*wi + i*(-vr*wi + vi*wr) */
    __m512d vro = _mm512_fmadd_pd(vr, wr_vec, _mm512_mul_pd(vi, wi_vec));
    __m512d vio = _mm512_fmsub_pd(vi, wr_vec, _mm512_mul_pd(vr, wi_vec));
    
    _mm512_storeu_pd(&wr[i1], _mm512_add_pd(ur, vro));
    _mm512_storeu_pd(&wi[i1], _mm512_add_pd(ui, vio));
    _mm512_storeu_pd(&wr[i2], _mm512_sub_pd(ur, vro));
    _mm512_storeu_pd(&wi[i2], _mm512_sub_pd(ui, vio));
}

/* Process single butterfly with AVX-512 (8 complex = 4 butterflies) */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly16_avx512(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    size_t i1, size_t i2,
    __m512d wr_vec, __m512d wi_vec
) {
    __m512d ur = _mm512_loadu_pd(&wr[i1]);
    __m512d ui = _mm512_loadu_pd(&wi[i1]);
    __m512d vr = _mm512_loadu_pd(&wr[i2]);
    __m512d vi = _mm512_loadu_pd(&wi[i2]);
    
    __m512d vro = _mm512_fmsub_pd(vr, wr_vec, _mm512_mul_pd(vi, wi_vec));
    __m512d vio = _mm512_fmadd_pd(vr, wi_vec, _mm512_mul_pd(vi, wr_vec));
    
    _mm512_storeu_pd(&wr[i1], _mm512_add_pd(ur, vro));
    _mm512_storeu_pd(&wi[i1], _mm512_add_pd(ui, vio));
    _mm512_storeu_pd(&wr[i2], _mm512_sub_pd(ur, vro));
    _mm512_storeu_pd(&wi[i2], _mm512_sub_pd(ui, vio));
}

#endif /* NOTORIOUS_FFT_HAS_AVX512 */

/* ============================================================================
 * AVX2 Implementation
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_AVX2

/* Process 4 elements (2 complex) at a time with gather */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly4_avx2(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT tw_re, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT tw_im,
    size_t i1, size_t i2, __m128i indices
) {
    __m256d ur = _mm256_loadu_pd(&wr[i1]);
    __m256d ui = _mm256_loadu_pd(&wi[i1]);
    __m256d vr = _mm256_loadu_pd(&wr[i2]);
    __m256d vi = _mm256_loadu_pd(&wi[i2]);
    
    /* Gather twiddles */
    __m256d t_wr = _mm256_i32gather_pd(tw_re, indices, 8);
    __m256d t_wi = _mm256_i32gather_pd(tw_im, indices, 8);
    
    /* Complex multiply */
    __m256d vro = _mm256_fmsub_pd(vr, t_wr, _mm256_mul_pd(vi, t_wi));
    __m256d vio = _mm256_fmadd_pd(vr, t_wi, _mm256_mul_pd(vi, t_wr));
    
    _mm256_storeu_pd(&wr[i1], _mm256_add_pd(ur, vro));
    _mm256_storeu_pd(&wi[i1], _mm256_add_pd(ui, vio));
    _mm256_storeu_pd(&wr[i2], _mm256_sub_pd(ur, vro));
    _mm256_storeu_pd(&wi[i2], _mm256_sub_pd(ui, vio));
}

/* Process single butterfly with AVX2 (4 complex = 2 butterflies) - INVERSE */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly4_avx2_inverse(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    size_t i1, size_t i2,
    __m256d wr_vec, __m256d wi_vec
) {
    __m256d ur = _mm256_loadu_pd(&wr[i1]);
    __m256d ui = _mm256_loadu_pd(&wi[i1]);
    __m256d vr = _mm256_loadu_pd(&wr[i2]);
    __m256d vi = _mm256_loadu_pd(&wi[i2]);
    
    /* For inverse: use conj(w) = (wr, -wi), so vr*wr + vi*wi + i*(-vr*wi + vi*wr) */
    __m256d vro = _mm256_fmadd_pd(vr, wr_vec, _mm256_mul_pd(vi, wi_vec));
    __m256d vio = _mm256_fmsub_pd(vi, wr_vec, _mm256_mul_pd(vr, wi_vec));
    
    _mm256_storeu_pd(&wr[i1], _mm256_add_pd(ur, vro));
    _mm256_storeu_pd(&wi[i1], _mm256_add_pd(ui, vio));
    _mm256_storeu_pd(&wr[i2], _mm256_sub_pd(ur, vro));
    _mm256_storeu_pd(&wi[i2], _mm256_sub_pd(ui, vio));
}

/* Process single butterfly with AVX2 (4 complex = 2 butterflies) */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly8_avx2(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    size_t i1, size_t i2,
    __m256d wr_vec, __m256d wi_vec
) {
    __m256d ur = _mm256_loadu_pd(&wr[i1]);
    __m256d ui = _mm256_loadu_pd(&wi[i1]);
    __m256d vr = _mm256_loadu_pd(&wr[i2]);
    __m256d vi = _mm256_loadu_pd(&wi[i2]);
    
    __m256d vro = _mm256_fmsub_pd(vr, wr_vec, _mm256_mul_pd(vi, wi_vec));
    __m256d vio = _mm256_fmadd_pd(vr, wi_vec, _mm256_mul_pd(vi, wr_vec));
    
    _mm256_storeu_pd(&wr[i1], _mm256_add_pd(ur, vro));
    _mm256_storeu_pd(&wi[i1], _mm256_add_pd(ui, vio));
    _mm256_storeu_pd(&wr[i2], _mm256_sub_pd(ur, vro));
    _mm256_storeu_pd(&wi[i2], _mm256_sub_pd(ui, vio));
}

#endif /* NOTORIOUS_FFT_HAS_AVX2 */

/* ============================================================================
 * NEON Implementation
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_NEON

#ifdef NOTORIOUS_FFT_SINGLE
/* Float32 version - 4 complex per operation */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly4_neon_f32(
    float* NOTORIOUS_FFT_RESTRICT wr, float* NOTORIOUS_FFT_RESTRICT wi,
    float wr0, float wi0, float wr1, float wi1, 
    float wr2, float wi2, float wr3, float wi3,
    size_t i1, size_t i2
) {
    float32x4_t ur = vld1q_f32(&wr[i1]);
    float32x4_t ui = vld1q_f32(&wi[i1]);
    float32x4_t vr = vld1q_f32(&wr[i2]);
    float32x4_t vi = vld1q_f32(&wi[i2]);
    
    float32x4_t t_wr = (float32x4_t){wr0, wr1, wr2, wr3};
    float32x4_t t_wi = (float32x4_t){wi0, wi1, wi2, wi3};
    
    float32x4_t vro = vsubq_f32(vmulq_f32(vr, t_wr), vmulq_f32(vi, t_wi));
    float32x4_t vio = vaddq_f32(vmulq_f32(vr, t_wi), vmulq_f32(vi, t_wr));
    
    vst1q_f32(&wr[i1], vaddq_f32(ur, vro));
    vst1q_f32(&wi[i1], vaddq_f32(ui, vio));
    vst1q_f32(&wr[i2], vsubq_f32(ur, vro));
    vst1q_f32(&wi[i2], vsubq_f32(ui, vio));
}
#else
/* Float64 version - 2 complex per operation */
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly2_neon_f64(
    double* NOTORIOUS_FFT_RESTRICT wr, double* NOTORIOUS_FFT_RESTRICT wi,
    double wr0, double wi0, double wr1, double wi1,
    size_t i1, size_t i2
) {
    float64x2_t ur = vld1q_f64(&wr[i1]);
    float64x2_t ui = vld1q_f64(&wi[i1]);
    float64x2_t vr = vld1q_f64(&wr[i2]);
    float64x2_t vi = vld1q_f64(&wi[i2]);
    
    float64x2_t t_wr = (float64x2_t){wr0, wr1};
    float64x2_t t_wi = (float64x2_t){wi0, wi1};
    
    float64x2_t vro = vsubq_f64(vmulq_f64(vr, t_wr), vmulq_f64(vi, t_wi));
    float64x2_t vio = vaddq_f64(vmulq_f64(vr, t_wi), vmulq_f64(vi, t_wr));
    
    vst1q_f64(&wr[i1], vaddq_f64(ur, vro));
    vst1q_f64(&wi[i1], vaddq_f64(ui, vio));
    vst1q_f64(&wr[i2], vsubq_f64(ur, vro));
    vst1q_f64(&wi[i2], vsubq_f64(ui, vio));
}
#endif /* NOTORIOUS_FFT_SINGLE */

/* ============================================================================
 * NEON Bluestein Algorithm Acceleration
 * ============================================================================ */

#ifndef NOTORIOUS_FFT_SINGLE

/* Vectorized complex multiply for pre-multiply by chirp (Step 1)
 * buf = x * chirp (complex pointwise multiplication)
 * N must be >= 2 for NEON path; scalar tail handles remainder */
static void notorious_fft_bluestein_premul_neon(size_t N,
                                          notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                          notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT x_re,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT x_im,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_re,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_im) {
    size_t i;
    /* Process 2 elements at a time using float64x2_t */
    for (i = 0; i + 2 <= N; i += 2) {
        float64x2_t xr = vld1q_f64(x_re + i);
        float64x2_t xi = vld1q_f64(x_im + i);
        float64x2_t cr = vld1q_f64(chirp_re + i);
        float64x2_t ci = vld1q_f64(chirp_im + i);

        /* buf_re = xr * cr - xi * ci */
        float64x2_t bre = vmlsq_f64(vmulq_f64(xr, cr), xi, ci);
        /* buf_im = xr * ci + xi * cr */
        float64x2_t bim = vfmaq_f64(vmulq_f64(xr, ci), xi, cr);

        vst1q_f64(buf_re + i, bre);
        vst1q_f64(buf_im + i, bim);
    }
    /* Scalar tail for remaining elements */
    for (; i < N; i++) {
        buf_re[i] = x_re[i] * chirp_re[i] - x_im[i] * chirp_im[i];
        buf_im[i] = x_re[i] * chirp_im[i] + x_im[i] * chirp_re[i];
    }
}

/* Vectorized complex multiply for frequency-domain convolution (Step 3)
 * buf = buf * chirp_fft (pointwise multiplication in freq domain)
 * M is the padded FFT size (power of 2, >= 2N-1) */
static void notorious_fft_bluestein_convolve_neon(size_t M,
                                            notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                            notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                            const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_fft_re,
                                            const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_fft_im) {
    size_t i;
    for (i = 0; i + 2 <= M; i += 2) {
        float64x2_t br = vld1q_f64(buf_re + i);
        float64x2_t bi = vld1q_f64(buf_im + i);
        float64x2_t cr = vld1q_f64(chirp_fft_re + i);
        float64x2_t ci = vld1q_f64(chirp_fft_im + i);

        /* new_re = br * cr - bi * ci */
        float64x2_t new_re = vmlsq_f64(vmulq_f64(br, cr), bi, ci);
        /* new_im = br * ci + bi * cr */
        float64x2_t new_im = vfmaq_f64(vmulq_f64(br, ci), bi, cr);

        vst1q_f64(buf_re + i, new_re);
        vst1q_f64(buf_im + i, new_im);
    }
    /* Scalar tail */
    for (; i < M; i++) {
        notorious_fft_real temp_re = buf_re[i] * chirp_fft_re[i] - buf_im[i] * chirp_fft_im[i];
        notorious_fft_real temp_im = buf_re[i] * chirp_fft_im[i] + buf_im[i] * chirp_fft_re[i];
        buf_re[i] = temp_re;
        buf_im[i] = temp_im;
    }
}

/* Vectorized complex multiply with scaling for post-multiply (Step 5)
 * out = buf * chirp * scale */
static void notorious_fft_bluestein_postmul_neon(size_t N,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT out_re,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT out_im,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_re,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_im,
                                           notorious_fft_real scale) {
    size_t i;
    float64x2_t vscale = vdupq_n_f64(scale);
    
    for (i = 0; i + 2 <= N; i += 2) {
        float64x2_t br = vld1q_f64(buf_re + i);
        float64x2_t bi = vld1q_f64(buf_im + i);
        float64x2_t cr = vld1q_f64(chirp_re + i);
        float64x2_t ci = vld1q_f64(chirp_im + i);

        /* out_re = (br * cr - bi * ci) * scale */
        float64x2_t ore = vmlsq_f64(vmulq_f64(br, cr), bi, ci);
        ore = vmulq_f64(ore, vscale);
        /* out_im = (br * ci + bi * cr) * scale */
        float64x2_t oim = vfmaq_f64(vmulq_f64(br, ci), bi, cr);
        oim = vmulq_f64(oim, vscale);

        vst1q_f64(out_re + i, ore);
        vst1q_f64(out_im + i, oim);
    }
    /* Scalar tail */
    for (; i < N; i++) {
        out_re[i] = (buf_re[i] * chirp_re[i] - buf_im[i] * chirp_im[i]) * scale;
        out_im[i] = (buf_re[i] * chirp_im[i] + buf_im[i] * chirp_re[i]) * scale;
    }
}

/* Zero-pad using NEON (set buf[n..m-1] to 0) */
static void notorious_fft_bluestein_zeropad_neon(size_t n, size_t m,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im) {
    size_t i;
    float64x2_t zero = vdupq_n_f64(0.0);
    
    for (i = n; i + 2 <= m; i += 2) {
        vst1q_f64(buf_re + i, zero);
        vst1q_f64(buf_im + i, zero);
    }
    for (; i < m; i++) {
        buf_re[i] = 0;
        buf_im[i] = 0;
    }
}

#endif /* !NOTORIOUS_FFT_SINGLE */

#endif /* NOTORIOUS_FFT_HAS_NEON */

/* ============================================================================
 * AVX2 Bluestein Algorithm Acceleration
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)

/* Vectorized complex multiply for pre-multiply by chirp (Step 1)
 * buf = x * chirp (complex pointwise multiplication)
 * Processes 4 elements per iteration using 256-bit AVX2 vectors. */
static void notorious_fft_bluestein_premul_avx2(size_t N,
                                          notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                          notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT x_re,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT x_im,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_re,
                                          const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_im) {
    size_t i;
    for (i = 0; i + 4 <= N; i += 4) {
        __m256d xr = _mm256_loadu_pd(x_re + i);
        __m256d xi = _mm256_loadu_pd(x_im + i);
        __m256d cr = _mm256_loadu_pd(chirp_re + i);
        __m256d ci = _mm256_loadu_pd(chirp_im + i);

        /* buf_re = xr * cr - xi * ci */
        __m256d bre = _mm256_fmsub_pd(xr, cr, _mm256_mul_pd(xi, ci));
        /* buf_im = xr * ci + xi * cr */
        __m256d bim = _mm256_fmadd_pd(xr, ci, _mm256_mul_pd(xi, cr));

        _mm256_storeu_pd(buf_re + i, bre);
        _mm256_storeu_pd(buf_im + i, bim);
    }
    /* Scalar tail */
    for (; i < N; i++) {
        buf_re[i] = x_re[i] * chirp_re[i] - x_im[i] * chirp_im[i];
        buf_im[i] = x_re[i] * chirp_im[i] + x_im[i] * chirp_re[i];
    }
}

/* Vectorized complex multiply for frequency-domain convolution (Step 3)
 * Processes 4 elements per iteration. */
static void notorious_fft_bluestein_convolve_avx2(size_t M,
                                            notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                            notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                            const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_fft_re,
                                            const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_fft_im) {
    size_t i;
    for (i = 0; i + 4 <= M; i += 4) {
        __m256d br = _mm256_loadu_pd(buf_re + i);
        __m256d bi = _mm256_loadu_pd(buf_im + i);
        __m256d cr = _mm256_loadu_pd(chirp_fft_re + i);
        __m256d ci = _mm256_loadu_pd(chirp_fft_im + i);

        __m256d new_re = _mm256_fmsub_pd(br, cr, _mm256_mul_pd(bi, ci));
        __m256d new_im = _mm256_fmadd_pd(br, ci, _mm256_mul_pd(bi, cr));

        _mm256_storeu_pd(buf_re + i, new_re);
        _mm256_storeu_pd(buf_im + i, new_im);
    }
    /* Scalar tail */
    for (; i < M; i++) {
        notorious_fft_real temp_re = buf_re[i] * chirp_fft_re[i] - buf_im[i] * chirp_fft_im[i];
        notorious_fft_real temp_im = buf_re[i] * chirp_fft_im[i] + buf_im[i] * chirp_fft_re[i];
        buf_re[i] = temp_re;
        buf_im[i] = temp_im;
    }
}

/* Vectorized complex multiply with scaling for post-multiply (Step 5) */
static void notorious_fft_bluestein_postmul_avx2(size_t N,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT out_re,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT out_im,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_re,
                                           const notorious_fft_real *NOTORIOUS_FFT_RESTRICT chirp_im,
                                           notorious_fft_real scale) {
    size_t i;
    __m256d vscale = _mm256_set1_pd(scale);

    for (i = 0; i + 4 <= N; i += 4) {
        __m256d br = _mm256_loadu_pd(buf_re + i);
        __m256d bi = _mm256_loadu_pd(buf_im + i);
        __m256d cr = _mm256_loadu_pd(chirp_re + i);
        __m256d ci = _mm256_loadu_pd(chirp_im + i);

        __m256d ore = _mm256_mul_pd(_mm256_fmsub_pd(br, cr, _mm256_mul_pd(bi, ci)), vscale);
        __m256d oim = _mm256_mul_pd(_mm256_fmadd_pd(br, ci, _mm256_mul_pd(bi, cr)), vscale);

        _mm256_storeu_pd(out_re + i, ore);
        _mm256_storeu_pd(out_im + i, oim);
    }
    /* Scalar tail */
    for (; i < N; i++) {
        out_re[i] = (buf_re[i] * chirp_re[i] - buf_im[i] * chirp_im[i]) * scale;
        out_im[i] = (buf_re[i] * chirp_im[i] + buf_im[i] * chirp_re[i]) * scale;
    }
}

/* Zero-pad using AVX2 */
static void notorious_fft_bluestein_zeropad_avx2(size_t n, size_t m,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_re,
                                           notorious_fft_real *NOTORIOUS_FFT_RESTRICT buf_im) {
    size_t i;
    __m256d zero = _mm256_setzero_pd();

    for (i = n; i + 4 <= m; i += 4) {
        _mm256_storeu_pd(buf_re + i, zero);
        _mm256_storeu_pd(buf_im + i, zero);
    }
    for (; i < m; i++) {
        buf_re[i] = 0;
        buf_im[i] = 0;
    }
}

#endif /* NOTORIOUS_FFT_HAS_AVX2 && !NOTORIOUS_FFT_SINGLE */

/* ============================================================================
 * SSE2 Implementation
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_SSE2

#ifdef NOTORIOUS_FFT_SINGLE
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly4_sse_f32(
    float* NOTORIOUS_FFT_RESTRICT wr, float* NOTORIOUS_FFT_RESTRICT wi,
    float wr0, float wi0, float wr1, float wi1,
    float wr2, float wi2, float wr3, float wi3,
    size_t i1, size_t i2
) {
    __m128 ur = _mm_loadu_ps(&wr[i1]);
    __m128 ui = _mm_loadu_ps(&wi[i1]);
    __m128 vr = _mm_loadu_ps(&wr[i2]);
    __m128 vi = _mm_loadu_ps(&wi[i2]);
    
    __m128 t_wr = _mm_set_ps(wr3, wr2, wr1, wr0);
    __m128 t_wi = _mm_set_ps(wi3, wi2, wi1, wi0);
    
    __m128 vro = _mm_sub_ps(_mm_mul_ps(vr, t_wr), _mm_mul_ps(vi, t_wi));
    __m128 vio = _mm_add_ps(_mm_mul_ps(vr, t_wi), _mm_mul_ps(vi, t_wr));
    
    _mm_storeu_ps(&wr[i1], _mm_add_ps(ur, vro));
    _mm_storeu_ps(&wi[i1], _mm_add_ps(ui, vio));
    _mm_storeu_ps(&wr[i2], _mm_sub_ps(ur, vro));
    _mm_storeu_ps(&wi[i2], _mm_sub_ps(ui, vio));
}
#else
static NOTORIOUS_FFT_INLINE void notorious_fft_butterfly2_sse_f64(
    double* NOTORIOUS_FFT_RESTRICT wr, double* NOTORIOUS_FFT_RESTRICT wi,
    double wr0, double wi0, double wr1, double wi1,
    size_t i1, size_t i2
) {
    __m128d ur = _mm_loadu_pd(&wr[i1]);
    __m128d ui = _mm_loadu_pd(&wi[i1]);
    __m128d vr = _mm_loadu_pd(&wr[i2]);
    __m128d vi = _mm_loadu_pd(&wi[i2]);
    
    __m128d t_wr = _mm_set_pd(wr1, wr0);
    __m128d t_wi = _mm_set_pd(wi1, wi0);
    
    __m128d vro = _mm_sub_pd(_mm_mul_pd(vr, t_wr), _mm_mul_pd(vi, t_wi));
    __m128d vio = _mm_add_pd(_mm_mul_pd(vr, t_wi), _mm_mul_pd(vi, t_wr));
    
    _mm_storeu_pd(&wr[i1], _mm_add_pd(ur, vro));
    _mm_storeu_pd(&wi[i1], _mm_add_pd(ui, vio));
    _mm_storeu_pd(&wr[i2], _mm_sub_pd(ur, vro));
    _mm_storeu_pd(&wi[i2], _mm_sub_pd(ui, vio));
}
#endif /* NOTORIOUS_FFT_SINGLE */

#endif /* NOTORIOUS_FFT_HAS_SSE2 */

/* ==========================================================================
 * 04_kernels.h
 * ========================================================================== */

/*
 * Notorious FFT - Hardcoded FFT Kernels for Small N
 * Fully unrolled for optimal performance
 */



/* ============================================================================
 * N = 2 Kernel
 * ============================================================================ */

static NOTORIOUS_FFT_INLINE void notorious_fft_kernel_2(notorious_fft_real* re, notorious_fft_real* im) {
    notorious_fft_real t0 = re[0] + re[1];
    notorious_fft_real t1 = re[0] - re[1];
    re[0] = t0; re[1] = t1;
    
    t0 = im[0] + im[1];
    t1 = im[0] - im[1];
    im[0] = t0; im[1] = t1;
}

/* ============================================================================
 * N = 4 Kernel - Fully Unrolled
 * ============================================================================ */

static NOTORIOUS_FFT_INLINE void notorious_fft_kernel_4(notorious_fft_real* re, notorious_fft_real* im) {
    /* Stage 1: Distance 2 butterflies (twiddle = 1) */
    notorious_fft_real t0 = re[0] + re[2];
    notorious_fft_real t1 = re[1] + re[3];
    notorious_fft_real t2 = re[0] - re[2];
    notorious_fft_real t3 = re[1] - re[3];
    re[0] = t0 + t1;
    re[2] = t0 - t1;
    re[1] = t2;
    re[3] = t3;
    
    t0 = im[0] + im[2];
    t1 = im[1] + im[3];
    t2 = im[0] - im[2];
    t3 = im[1] - im[3];
    im[0] = t0 + t1;
    im[2] = t0 - t1;
    im[1] = t2;
    im[3] = t3;
    
    /* Stage 2: Distance 1 butterflies
     * Pair (0,1): twiddle = 1
     * Pair (2,3): twiddle = -i (swap and negate)
     */
    t0 = re[1];
    t1 = im[1];
    t2 = re[3];  /* save re[3] before overwrite */
    t3 = im[3];  /* save im[3] before overwrite */
    re[1] = t0 + t3;
    im[1] = t1 - t2;
    re[3] = t0 - t3;
    im[3] = t1 + t2;
}

/* ============================================================================
 * N = 8 Kernel - Optimized with explicit twiddles (FORWARD)
 * ============================================================================ */

static NOTORIOUS_FFT_INLINE void notorious_fft_kernel_8(notorious_fft_real* re, notorious_fft_real* im) {
    const notorious_fft_real c4 = NOTORIOUS_FFT_INV_SQRT2;  /* cos(π/4) = sin(π/4) */
    
    /* Stage 1: Distance 4 butterflies (twiddle = 1) */
    notorious_fft_real ar0 = re[0] + re[4], ar1 = re[1] + re[5];
    notorious_fft_real ar2 = re[2] + re[6], ar3 = re[3] + re[7];
    notorious_fft_real br0 = re[0] - re[4], br1 = re[1] - re[5];
    notorious_fft_real br2 = re[2] - re[6], br3 = re[3] - re[7];
    
    notorious_fft_real ai0 = im[0] + im[4], ai1 = im[1] + im[5];
    notorious_fft_real ai2 = im[2] + im[6], ai3 = im[3] + im[7];
    notorious_fft_real bi0 = im[0] - im[4], bi1 = im[1] - im[5];
    notorious_fft_real bi2 = im[2] - im[6], bi3 = im[3] - im[7];
    
    /* Stage 2: Distance 2 butterflies
     * Groups 0-3: twiddle = 1
     * Groups 4-7: twiddle = -i
     */
    notorious_fft_real cr0 = ar0 + ar2, cr1 = ar1 + ar3;
    notorious_fft_real cr2 = ar0 - ar2, cr3 = ar1 - ar3;
    notorious_fft_real ci0 = ai0 + ai2, ci1 = ai1 + ai3;
    notorious_fft_real ci2 = ai0 - ai2, ci3 = ai1 - ai3;
    
    notorious_fft_real dr0 = br0 + bi2, dr1 = br1 + bi3;
    notorious_fft_real dr2 = br0 - bi2, dr3 = br1 - bi3;
    notorious_fft_real di0 = bi0 - br2, di1 = bi1 - br3;
    notorious_fft_real di2 = bi0 + br2, di3 = bi1 + br3;
    
    /* Stage 3: Distance 1 butterflies with various twiddles */
    /* Index 0,1: twiddle = 1 */
    re[0] = cr0 + cr1; im[0] = ci0 + ci1;
    re[1] = cr0 - cr1; im[1] = ci0 - ci1;
    
    /* Index 2,3: twiddle = exp(-iπ/4) = c4 - i*c4 */
    /* (cr2 + i*ci2) + (cr3 + i*ci3)*(c4 - i*c4) */
    notorious_fft_real tr = cr3 * c4 + ci3 * c4;  /* Real part of product */
    notorious_fft_real ti = ci3 * c4 - cr3 * c4;  /* Imag part of product */
    re[2] = cr2 + tr; im[2] = ci2 + ti;
    re[3] = cr2 - tr; im[3] = ci2 - ti;
    
    /* Index 4,5: twiddle = exp(-iπ/2) = -i */
    re[4] = dr0 + di1; im[4] = di0 - dr1;
    re[5] = dr0 - di1; im[5] = di0 + dr1;
    
    /* Index 6,7: twiddle = exp(-i3π/4) = -c4 - i*c4 */
    tr = -dr3 * c4 + di3 * c4;
    ti = -dr3 * c4 - di3 * c4;
    re[6] = dr2 + tr; im[6] = di2 + ti;
    re[7] = dr2 - tr; im[7] = di2 - ti;
}

/* ==========================================================================
 * 05_algorithms.h
 * ========================================================================== */

/*
 * Notorious FFT - Core FFT Algorithms
 * Iterative, cache-oblivious recursive, and Bluestein
 */



/* Forward declarations */
static void notorious_fft_execute_sr_dif(const notorious_fft_plan* plan, const notorious_fft_real* x_in,
                                   notorious_fft_real* y_out, int inverse);
static void notorious_fft_sr_dif_cx(int N, notorious_fft_real* x, notorious_fft_real* t,
                               notorious_fft_real* y, int sy, const notorious_fft_real* e);
static void notorious_fft_sr_inv_dif_cx(int N, notorious_fft_real* x, notorious_fft_real* t,
                                   notorious_fft_real* y, int sy, const notorious_fft_real* e);
static void notorious_fft_execute_bluestein(
    const notorious_fft_plan* plan,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_in, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_in,
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_out, notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_out);

/* ============================================================================
 * Iterative Cooley-Tukey FFT
 * ============================================================================ */

/* Internal iterative FFT with direction control (0=forward, 1=inverse) */
static void notorious_fft_iterative_body_internal(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT wr, notorious_fft_real* NOTORIOUS_FFT_RESTRICT wi,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_in, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_in,
    const int* bitrev,
    const notorious_fft_real* tw_re, const notorious_fft_real* tw_im,
    size_t n,
    int inverse)
{
    /* Bit-reversal permutation */
    for (size_t i = 0; i < n; i++) {
        size_t j = bitrev[i];
        wr[i] = xr_in[j];
        wi[i] = xi_in[j];
    }
    
    /* Iterative butterfly stages */
    for (size_t len = 2; len <= n; len <<= 1) {
        size_t half = len >> 1;
        size_t step = n / len;
        
        #if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n >= 4096 && len >= 32)
        #endif
        for (size_t i = 0; i < n; i += len) {
            size_t j = 0;
            
            #if NOTORIOUS_FFT_HAS_AVX512
            {
                int32_t indices[8];
                for (; j + 8 <= half; j += 8) {
                    for (int k = 0; k < 8; k++) indices[k] = (int32_t)((j + k) * step);
                    if (inverse) {
                        /* For inverse, negate twiddle imag parts after gather */
                        __m256i idx = _mm256_loadu_si256((__m256i*)indices);
                        __m512d t_wr = _mm512_i32gather_pd(idx, tw_re, 8);
                        __m512d t_wi = _mm512_i32gather_pd(idx, tw_im, 8);
                        t_wi = _mm512_sub_pd(_mm512_setzero_pd(), t_wi); /* negate */
                        notorious_fft_butterfly8_avx512_inverse(wr, wi, i + j, i + j + half, t_wr, t_wi);
                    } else {
                        notorious_fft_butterfly8_avx512(wr, wi, tw_re, tw_im, i + j, i + j + half, indices);
                    }
                }
            }
            #elif NOTORIOUS_FFT_HAS_AVX2
            {
                int32_t idx[4];
                for (; j + 4 <= half; j += 4) {
                    for (int k = 0; k < 4; k++) idx[k] = (int32_t)((j + k) * step);
                    __m128i indices = _mm_loadu_si128((__m128i*)idx);
                    if (inverse) {
                        /* For inverse, negate twiddle imag parts after gather */
                        __m256d t_wr = _mm256_i32gather_pd(tw_re, indices, 8);
                        __m256d t_wi = _mm256_i32gather_pd(tw_im, indices, 8);
                        t_wi = _mm256_sub_pd(_mm256_setzero_pd(), t_wi); /* negate */
                        notorious_fft_butterfly4_avx2_inverse(wr, wi, i + j, i + j + half, t_wr, t_wi);
                    } else {
                        notorious_fft_butterfly4_avx2(wr, wi, tw_re, tw_im, i + j, i + j + half, indices);
                    }
                }
            }
            #elif NOTORIOUS_FFT_HAS_NEON
            {
                #ifdef NOTORIOUS_FFT_SINGLE
                for (; j + 4 <= half; j += 4) {
                    float wr0 = tw_re[(j+0)*step], wi0 = inverse ? -tw_im[(j+0)*step] : tw_im[(j+0)*step];
                    float wr1 = tw_re[(j+1)*step], wi1 = inverse ? -tw_im[(j+1)*step] : tw_im[(j+1)*step];
                    float wr2 = tw_re[(j+2)*step], wi2 = inverse ? -tw_im[(j+2)*step] : tw_im[(j+2)*step];
                    float wr3 = tw_re[(j+3)*step], wi3 = inverse ? -tw_im[(j+3)*step] : tw_im[(j+3)*step];
                    notorious_fft_butterfly4_neon_f32(wr, wi, wr0, wi0, wr1, wi1, wr2, wi2, wr3, wi3,
                                                i + j, i + j + half);
                }
                #else
                for (; j + 2 <= half; j += 2) {
                    double wr0 = tw_re[(j+0)*step], wi0 = inverse ? -tw_im[(j+0)*step] : tw_im[(j+0)*step];
                    double wr1 = tw_re[(j+1)*step], wi1 = inverse ? -tw_im[(j+1)*step] : tw_im[(j+1)*step];
                    notorious_fft_butterfly2_neon_f64(wr, wi, wr0, wi0, wr1, wi1, i + j, i + j + half);
                }
                #endif
            }
            #endif
            
            /* Scalar cleanup */
            for (; j < half; j++) {
                notorious_fft_real t_wr = tw_re[j * step];
                notorious_fft_real t_wi = inverse ? -tw_im[j * step] : tw_im[j * step];
                notorious_fft_butterfly_scalar(wr, wi, i + j, i + j + half, t_wr, t_wi);
            }
        }
    }
}

/* Internal function with direction control (0=forward, 1=inverse) */
static void notorious_fft_execute_iterative_internal(
    const notorious_fft_plan* plan,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_in, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_in,
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_out, notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_out,
    int inverse)
{
    size_t n = plan->n;
    
    /* Use iterative algorithm - hardcoded kernels have bugs */
    notorious_fft_real* wr = plan->work_re;
    notorious_fft_real* wi = plan->work_im;
    
    notorious_fft_iterative_body_internal(wr, wi, xr_in, xi_in, plan->bitrev, plan->tw_re, plan->tw_im, n, inverse);
    
    memcpy(xr_out, wr, n * sizeof(notorious_fft_real));
    memcpy(xi_out, wi, n * sizeof(notorious_fft_real));
}

/* Public API - forward FFT only (keeps original signature) */
static void notorious_fft_execute_iterative(
    const notorious_fft_plan* plan,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_in, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_in,
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_out, notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_out)
{
    notorious_fft_execute_iterative_internal(plan, xr_in, xi_in, xr_out, xi_out, 0);
}

/* ============================================================================
 * In-place iterative FFT on interleaved complex (re,im pairs)
 *
 * Avoids the split-complex deinterleave/interleave round-trip in the
 * minfft-compatible API.  Operates directly on the notorious_fft_cmpl* (double[2])
 * array that the caller already has.
 * ============================================================================ */

static void notorious_fft_iterative_inplace_cx(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT data,  /* interleaved: data[2*i]=re, data[2*i+1]=im */
    const int*         bitrev,
    const notorious_fft_real* tw_re,
    const notorious_fft_real* tw_im,
    size_t n,
    int inverse)
{
    /* Fast path: use proven split-radix terminal cases for small N.
     * This avoids bit-reversal overhead which is the main cost for small FFTs.
     * The split-radix N=8 terminal case is extensively tested and correct. */
    if (n == 8) {
        /* Use split-radix N=8 terminal case via notorious_fft_sr_dif_cx.
         * Need temp buffer for the butterfly stage output. */
        notorious_fft_real t[16];  /* 8 complex = 16 reals */
        if (inverse) {
            notorious_fft_sr_inv_dif_cx(8, data, t, data, 1, NULL);
        } else {
            notorious_fft_sr_dif_cx(8, data, t, data, 1, NULL);
        }
        return;
    }

    /* Fast path: N=4 using split-radix terminal case */
    if (n == 4) {
        notorious_fft_real t[8];  /* 4 complex = 8 reals */
        if (inverse) {
            notorious_fft_sr_inv_dif_cx(4, data, t, data, 1, NULL);
        } else {
            notorious_fft_sr_dif_cx(4, data, t, data, 1, NULL);
        }
        return;
    }

    /* Fast path: N=2 using split-radix terminal case */
    if (n == 2) {
        notorious_fft_real t[4];  /* 2 complex = 4 reals */
        if (inverse) {
            notorious_fft_sr_inv_dif_cx(2, data, t, data, 1, NULL);
        } else {
            notorious_fft_sr_dif_cx(2, data, t, data, 1, NULL);
        }
        return;
    }

    /* Bit-reversal permutation (swap pairs) */
    for (size_t i = 0; i < n; i++) {
        size_t j = (size_t)bitrev[i];
        if (j > i) {
            notorious_fft_real tr = data[2*i],   ti = data[2*i+1];
            data[2*i]   = data[2*j];   data[2*i+1] = data[2*j+1];
            data[2*j]   = tr;          data[2*j+1] = ti;
        }
    }

    /* Iterative Cooley–Tukey stages */
    for (size_t len = 2; len <= n; len <<= 1) {
        size_t half = len >> 1;
        size_t step = n / len;

#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n >= 4096 && len >= 32)
#endif
        for (size_t i = 0; i < n; i += len) {
            size_t j = 0;

#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
            /* AVX2 double: process 2 complex per iteration on interleaved data */
            for (; j + 2 <= half; j += 2) {
                __m256d tw_r, tw_i;
                {
                    double wr0 = tw_re[(j+0)*step], wr1 = tw_re[(j+1)*step];
                    double wi0 = tw_im[(j+0)*step], wi1 = tw_im[(j+1)*step];
                    if (inverse) { wi0 = -wi0; wi1 = -wi1; }
                    tw_r = _mm256_set_pd(wr1, wr1, wr0, wr0);
                    tw_i = _mm256_set_pd(wi1, wi1, wi0, wi0);
                }

                __m256d ab = _mm256_loadu_pd(&data[2*(i+j)]);
                __m256d cd = _mm256_loadu_pd(&data[2*(i+j+half)]);

                /* Complex multiply cd * tw on interleaved data */
                __m256d cd_swap = _mm256_shuffle_pd(cd, cd, 0x5);
                __m256d cmul_sign = _mm256_set_pd(1.0, -1.0, 1.0, -1.0);
                __m256d p1 = _mm256_mul_pd(cd, tw_r);
                __m256d p2 = _mm256_mul_pd(cd_swap, tw_i);
                __m256d prod = _mm256_add_pd(p1, _mm256_mul_pd(p2, cmul_sign));

                _mm256_storeu_pd(&data[2*(i+j)],      _mm256_add_pd(ab, prod));
                _mm256_storeu_pd(&data[2*(i+j+half)],  _mm256_sub_pd(ab, prod));
            }
#elif NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
            /* NEON double: vld2q_f64 deinterleaves in hardware */
            for (; j + 2 <= half; j += 2) {
                float64x2_t tw_r = (float64x2_t){tw_re[(j+0)*step], tw_re[(j+1)*step]};
                float64x2_t tw_i = inverse
                    ? (float64x2_t){-tw_im[(j+0)*step], -tw_im[(j+1)*step]}
                    : (float64x2_t){ tw_im[(j+0)*step],  tw_im[(j+1)*step]};

                float64x2x2_t ab = vld2q_f64(&data[2*(i+j)]);
                float64x2x2_t cd = vld2q_f64(&data[2*(i+j+half)]);

                float64x2_t vr = vsubq_f64(vmulq_f64(cd.val[0], tw_r), vmulq_f64(cd.val[1], tw_i));
                float64x2_t vi = vaddq_f64(vmulq_f64(cd.val[0], tw_i), vmulq_f64(cd.val[1], tw_r));

                float64x2x2_t out_p, out_q;
                out_p.val[0] = vaddq_f64(ab.val[0], vr);
                out_p.val[1] = vaddq_f64(ab.val[1], vi);
                out_q.val[0] = vsubq_f64(ab.val[0], vr);
                out_q.val[1] = vsubq_f64(ab.val[1], vi);

                vst2q_f64(&data[2*(i+j)],        out_p);
                vst2q_f64(&data[2*(i+j+half)],   out_q);
            }
#endif
            /* Scalar remainder */
            for (; j < half; j++) {
                notorious_fft_real wr = tw_re[j * step];
                notorious_fft_real wi = inverse ? -tw_im[j * step] : tw_im[j * step];

                size_t p = 2 * (i + j);
                size_t q = 2 * (i + j + half);

                notorious_fft_real ur = data[p],   ui = data[p+1];
                notorious_fft_real vr = data[q] * wr - data[q+1] * wi;
                notorious_fft_real vi = data[q] * wi + data[q+1] * wr;

                data[p]   = ur + vr;  data[p+1] = ui + vi;
                data[q]   = ur - vr;  data[q+1] = ui - vi;
            }
        }
    }
}

/* Wrapper that accepts separate in/out buffers.
 * Routes to split-radix DIF (no bit-reversal, better cache) for large N
 * where sr_e/sr_t are available, otherwise falls back to iterative DIT. */
static void notorious_fft_execute_cx(
    const notorious_fft_plan* plan,
    const notorious_fft_real* x_in,   /* interleaved input  */
    notorious_fft_real*       y_out,  /* interleaved output */
    int inverse)
{
    size_t n = plan->n;

    /* Bluestein (non-power-of-2): deinterleave → split execute → reinterleave.
     * work_re/im are not used by notorious_fft_execute_bluestein, safe for input.
     * bluestein_fft_buf_re/im are free after execute returns, safe for output. */
    if (plan->execute_func == notorious_fft_execute_bluestein) {
        notorious_fft_real* in_re = plan->work_re;
        notorious_fft_real* in_im = plan->work_im;
        notorious_fft_real* out_re = plan->bluestein_fft_buf_re;
        notorious_fft_real* out_im = plan->bluestein_fft_buf_im;

        /* Deinterleave input into split re/im */
        for (size_t i = 0; i < n; i++) {
            in_re[i] = x_in[2*i];
            in_im[i] = x_in[2*i+1];
        }

        /* Handle inverse via conjugate trick: IDFT(x) = (1/N)*conj(DFT(conj(x))) */
        if (inverse) {
            for (size_t i = 0; i < n; i++) in_im[i] = -in_im[i];
        }

        notorious_fft_execute_bluestein(plan, in_re, in_im, out_re, out_im);

        if (inverse) {
            notorious_fft_real scale = (notorious_fft_real)1.0 / (notorious_fft_real)n;
            for (size_t i = 0; i < n; i++) {
                y_out[2*i]   =  out_re[i] * scale;
                y_out[2*i+1] = -out_im[i] * scale;
            }
        } else {
            for (size_t i = 0; i < n; i++) {
                y_out[2*i]   = out_re[i];
                y_out[2*i+1] = out_im[i];
            }
        }
        return;
    }

    if (plan->sr_e && plan->sr_t && n >= 16) {
        notorious_fft_execute_sr_dif(plan, x_in, y_out, inverse);
    } else {
        if (x_in != y_out)
            memcpy(y_out, x_in, 2 * n * sizeof(notorious_fft_real));
        notorious_fft_iterative_inplace_cx(y_out, plan->bitrev, plan->tw_re, plan->tw_im, n, inverse);
    }
}

/* ============================================================================
 * Split-Radix DIF (Decimation-In-Frequency) — minfft-compatible algorithm
 *
 * Works directly on interleaved complex data (re,im pairs), no bit-reversal
 * needed.  Same split-radix 2/4 structure as minfft's rs_dft_1d.
 *
 * Twiddle layout in sr_e (same as minfft):
 *   For each recursion level of size N (N≥16, processed from large to small):
 *     N/4 quads: { cos(-k·2π/N), sin(-k·2π/N),
 *                  cos(-3k·2π/N), sin(-3k·2π/N) }   k = 0..N/4-1
 * ============================================================================ */

static void notorious_fft_sr_dif_cx(int N, notorious_fft_real* x, notorious_fft_real* t,
                               notorious_fft_real* y, int sy,
                               const notorious_fft_real* e)
{
    notorious_fft_real* xr = x, *xi = x + 1;
    notorious_fft_real* tr = t, *ti = t + 1;
    notorious_fft_real* yr = y, *yi = y + 1;

    if (N == 1) {
        yr[0] = xr[0]; yi[0] = xi[0];
        return;
    }
    if (N == 2) {
        notorious_fft_real t0r = xr[0] + xr[2], t0i = xi[0] + xi[2];
        notorious_fft_real t1r = xr[0] - xr[2], t1i = xi[0] - xi[2];
        yr[0]      = t0r; yi[0]      = t0i;
        yr[2*sy]   = t1r; yi[2*sy]   = t1i;
        return;
    }
    if (N == 4) {
        notorious_fft_real t0r = xr[0] + xr[4], t0i = xi[0] + xi[4];
        notorious_fft_real t1r = xr[2] + xr[6], t1i = xi[2] + xi[6];
        notorious_fft_real t2r = xr[0] - xr[4], t2i = xi[0] - xi[4];
        /* t3 = i*(x[1]-x[3]) */
        notorious_fft_real t3r = -xi[2] + xi[6], t3i = xr[2] - xr[6];
        yr[0]      = t0r + t1r; yi[0]      = t0i + t1i;
        yr[2*sy]   = t2r - t3r; yi[2*sy]   = t2i - t3i;
        yr[4*sy]   = t0r - t1r; yi[4*sy]   = t0i - t1i;
        yr[6*sy]   = t2r + t3r; yi[6*sy]   = t2i + t3i;
        return;
    }
    if (N == 8) {
        /* Unrolled N=8 split-radix — identical to minfft terminal case */
        notorious_fft_real t0r,t0i,t1r,t1i,t2r,t2i,t3r,t3i;
        notorious_fft_real t00r,t00i,t01r,t01i,t02r,t02i,t03r,t03i;
        notorious_fft_real t10r,t10i,t11r,t11i,t12r,t12i,t13r,t13i;
        notorious_fft_real ttr,tti;
        const notorious_fft_real invsqrt2 = NOTORIOUS_FFT_INV_SQRT2;

        t0r=xr[0]+xr[8];  t0i=xi[0]+xi[8];
        t1r=xr[4]+xr[12]; t1i=xi[4]+xi[12];
        t2r=xr[0]-xr[8];  t2i=xi[0]-xi[8];
        t3r=-xi[4]+xi[12]; t3i=xr[4]-xr[12];
        t00r=t0r+t1r; t00i=t0i+t1i;
        t01r=t2r-t3r; t01i=t2i-t3i;
        t02r=t0r-t1r; t02i=t0i-t1i;
        t03r=t2r+t3r; t03i=t2i+t3i;

        t0r=xr[2]+xr[10]; t0i=xi[2]+xi[10];
        t1r=xr[6]+xr[14]; t1i=xi[6]+xi[14];
        t2r=xr[2]-xr[10]; t2i=xi[2]-xi[10];
        t3r=-xi[6]+xi[14]; t3i=xr[6]-xr[14];

        t10r=t0r+t1r; t10i=t0i+t1i;
        ttr=t2r-t3r; tti=t2i-t3i;
        t11r=invsqrt2*(ttr+tti); t11i=invsqrt2*(tti-ttr);
        t12r=t0i-t1i; t12i=-t0r+t1r;
        ttr=t2r+t3r; tti=t2i+t3i;
        t13r=invsqrt2*(tti-ttr); t13i=-invsqrt2*(tti+ttr);

        yr[0]=t00r+t10r;    yi[0]=t00i+t10i;
        yr[2*sy]=t01r+t11r; yi[2*sy]=t01i+t11i;
        yr[4*sy]=t02r+t12r; yi[4*sy]=t02i+t12i;
        yr[6*sy]=t03r+t13r; yi[6*sy]=t03i+t13i;
        yr[8*sy]=t00r-t10r; yi[8*sy]=t00i-t10i;
        yr[10*sy]=t01r-t11r; yi[10*sy]=t01i-t11i;
        yr[12*sy]=t02r-t12r; yi[12*sy]=t02i-t12i;
        yr[14*sy]=t03r-t13r; yi[14*sy]=t03i-t13i;
        return;
    }

    /* General recursion: split-radix DIF butterfly stage then recurse */
    /* N >= 16 */
    int n4 = N / 4;
    const notorious_fft_real* ep = e;  /* points to current level's twiddles */

    {
        int k = 0;

#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
        /* AVX2 double: process 2 complex per iteration using 256-bit ops.
         * Load 4 doubles {re0,im0,re1,im1}, deinterleave with permute. */
        for (; k + 2 <= n4; k += 2) {
            /* Twiddles: gather from ep[4*k..] layout */
            __m256d w1r = _mm256_set_pd(ep[4*(k+1)],   ep[4*(k+1)],   ep[4*k],   ep[4*k]);
            __m256d w1i = _mm256_set_pd(ep[4*(k+1)+1], ep[4*(k+1)+1], ep[4*k+1], ep[4*k+1]);
            __m256d w3r = _mm256_set_pd(ep[4*(k+1)+2], ep[4*(k+1)+2], ep[4*k+2], ep[4*k+2]);
            __m256d w3i = _mm256_set_pd(ep[4*(k+1)+3], ep[4*(k+1)+3], ep[4*k+3], ep[4*k+3]);

            /* Load 2 complex values each: {re0,im0,re1,im1} */
            __m256d a = _mm256_loadu_pd(xr + 2*k);
            __m256d b = _mm256_loadu_pd(xr + 2*(k+N/2));
            __m256d c = _mm256_loadu_pd(xr + 2*(k+n4));
            __m256d d = _mm256_loadu_pd(xr + 2*(k+3*n4));

            __m256d t0 = _mm256_add_pd(a, b);  /* a+b interleaved */
            __m256d t1 = _mm256_add_pd(c, d);  /* c+d interleaved */
            __m256d t2 = _mm256_sub_pd(a, b);  /* a-b interleaved */
            __m256d cd_diff = _mm256_sub_pd(c, d);

            /* t3 = i*(c-d): swap re/im and negate new re
             * cd_diff = {re0,im0,re1,im1} → t3 = {-im0,re0,-im1,re1} */
            __m256d cd_swap = _mm256_shuffle_pd(cd_diff, cd_diff, 0x5); /* {im0,re0,im1,re1} */
            __m256d sign_mask = _mm256_set_pd(1.0, -1.0, 1.0, -1.0);
            __m256d t3 = _mm256_mul_pd(cd_swap, sign_mask);

            __m256d u = _mm256_sub_pd(t2, t3);
            __m256d v = _mm256_add_pd(t2, t3);

            _mm256_storeu_pd(tr + 2*k, t0);
            _mm256_storeu_pd(tr + 2*(k+n4), t1);

            /* u * w1: complex multiply on interleaved data
             * u = {ur0,ui0,ur1,ui1}, w1r = {wr0,wr0,wr1,wr1}, w1i = {wi0,wi0,wi1,wi1}
             * result_re = ur*wr - ui*wi, result_im = ur*wi + ui*wr */
            __m256d u_swap = _mm256_shuffle_pd(u, u, 0x5); /* {ui0,ur0,ui1,ur1} */
            __m256d p1 = _mm256_mul_pd(u, w1r);           /* {ur*wr, ui*wr, ...} */
            __m256d p2 = _mm256_mul_pd(u_swap, w1i);      /* {ui*wi, ur*wi, ...} */
            __m256d cmul_sign = _mm256_set_pd(1.0, -1.0, 1.0, -1.0);
            __m256d uw1 = _mm256_add_pd(p1, _mm256_mul_pd(p2, cmul_sign));

            _mm256_storeu_pd(tr + 2*(k+N/2), uw1);

            /* v * w3: same complex multiply pattern */
            __m256d v_swap = _mm256_shuffle_pd(v, v, 0x5);
            __m256d q1 = _mm256_mul_pd(v, w3r);
            __m256d q2 = _mm256_mul_pd(v_swap, w3i);
            __m256d vw3 = _mm256_add_pd(q1, _mm256_mul_pd(q2, cmul_sign));

            _mm256_storeu_pd(tr + 2*(k+3*n4), vw3);
        }

#elif NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
        /* NEON double: process 2 complex per iteration using vld2q_f64 deinterleave */
        for (; k + 2 <= n4; k += 2) {
            float64x2_t w1r = (float64x2_t){ep[4*k],   ep[4*(k+1)]};
            float64x2_t w1i = (float64x2_t){ep[4*k+1], ep[4*(k+1)+1]};
            float64x2_t w3r = (float64x2_t){ep[4*k+2], ep[4*(k+1)+2]};
            float64x2_t w3i = (float64x2_t){ep[4*k+3], ep[4*(k+1)+3]};

            float64x2x2_t xa = vld2q_f64(xr + 2*k);
            float64x2x2_t xb = vld2q_f64(xr + 2*(k+N/2));
            float64x2x2_t xc = vld2q_f64(xr + 2*(k+n4));
            float64x2x2_t xd = vld2q_f64(xr + 2*(k+3*n4));

            float64x2_t t0r_ = vaddq_f64(xa.val[0], xb.val[0]);
            float64x2_t t0i_ = vaddq_f64(xa.val[1], xb.val[1]);
            float64x2_t t1r_ = vaddq_f64(xc.val[0], xd.val[0]);
            float64x2_t t1i_ = vaddq_f64(xc.val[1], xd.val[1]);
            float64x2_t t2r_ = vsubq_f64(xa.val[0], xb.val[0]);
            float64x2_t t2i_ = vsubq_f64(xa.val[1], xb.val[1]);
            float64x2_t t3r_ = vsubq_f64(xd.val[1], xc.val[1]);
            float64x2_t t3i_ = vsubq_f64(xc.val[0], xd.val[0]);

            float64x2_t ur_ = vsubq_f64(t2r_, t3r_), ui_ = vsubq_f64(t2i_, t3i_);
            float64x2_t vr_ = vaddq_f64(t2r_, t3r_), vi_ = vaddq_f64(t2i_, t3i_);

            float64x2x2_t out0 = {{t0r_, t0i_}};
            vst2q_f64(tr + 2*k, out0);
            float64x2x2_t out1 = {{t1r_, t1i_}};
            vst2q_f64(tr + 2*(k+n4), out1);

            float64x2_t pr_ = vsubq_f64(vmulq_f64(ur_,w1r), vmulq_f64(ui_,w1i));
            float64x2_t pi_ = vaddq_f64(vmulq_f64(ur_,w1i), vmulq_f64(ui_,w1r));
            float64x2x2_t out2 = {{pr_, pi_}};
            vst2q_f64(tr + 2*(k+N/2), out2);

            float64x2_t qr_ = vsubq_f64(vmulq_f64(vr_,w3r), vmulq_f64(vi_,w3i));
            float64x2_t qi_ = vaddq_f64(vmulq_f64(vr_,w3i), vmulq_f64(vi_,w3r));
            float64x2x2_t out3 = {{qr_, qi_}};
            vst2q_f64(tr + 2*(k+3*n4), out3);
        }
#endif

        /* Scalar remainder / fallback */
        for (; k < n4; k++) {
            notorious_fft_real x0r=xr[2*k],x0i=xi[2*k];
            notorious_fft_real x1r=xr[2*(k+N/2)],x1i=xi[2*(k+N/2)];
            notorious_fft_real x2r=xr[2*(k+n4)],x2i=xi[2*(k+n4)];
            notorious_fft_real x3r=xr[2*(k+3*n4)],x3i=xi[2*(k+3*n4)];
            notorious_fft_real t0r_=x0r+x1r,t0i_=x0i+x1i;
            notorious_fft_real t1r_=x2r+x3r,t1i_=x2i+x3i;
            notorious_fft_real t2r_=x0r-x1r,t2i_=x0i-x1i;
            notorious_fft_real t3r_=x3i-x2i,t3i_=x2r-x3r;
            notorious_fft_real ur_=t2r_-t3r_,ui_=t2i_-t3i_;
            notorious_fft_real vr_=t2r_+t3r_,vi_=t2i_+t3i_;
            tr[2*k]=t0r_; ti[2*k]=t0i_;
            tr[2*(k+n4)]=t1r_; ti[2*(k+n4)]=t1i_;
            tr[2*(k+N/2)]=ur_*ep[4*k]-ui_*ep[4*k+1];
            ti[2*(k+N/2)]=ur_*ep[4*k+1]+ui_*ep[4*k];
            tr[2*(k+3*n4)]=vr_*ep[4*k+2]-vi_*ep[4*k+3];
            ti[2*(k+3*n4)]=vr_*ep[4*k+3]+vi_*ep[4*k+2];
        }
    }

    /* e pointer advances by N/2 pairs = N reals to skip to next level */
    /* e_next: skip current level's N/4 quads × 4 reals = N reals */
    const notorious_fft_real* e_next = e + N;

    /* t offsets are in reals (interleaved: each complex = 2 reals).
     * t[k+N/2] starts at t + 2*(N/2) = t + N reals.
     * t[k+3N/4] starts at t + 2*(3N/4) = t + 3N/2 reals. */
    notorious_fft_sr_dif_cx(N/2, t,          t,          y,       2*sy, e_next);
    notorious_fft_sr_dif_cx(N/4, t+N,        t+N,        y+2*sy,  4*sy, e_next + N/2);
    notorious_fft_sr_dif_cx(N/4, t+3*(N/2),  t+3*(N/2),  y+6*sy,  4*sy, e_next + N/2);
}

/* Inverse split-radix DIF — uses conj(e) twiddles and swapped +/- t3 */
static void notorious_fft_sr_inv_dif_cx(int N, notorious_fft_real* x, notorious_fft_real* t,
                                   notorious_fft_real* y, int sy,
                                   const notorious_fft_real* e)
{
    notorious_fft_real* xr = x, *xi = x + 1;
    notorious_fft_real* tr = t, *ti = t + 1;
    notorious_fft_real* yr = y, *yi = y + 1;

    if (N == 1) {
        yr[0] = xr[0]; yi[0] = xi[0];
        return;
    }
    if (N == 2) {
        notorious_fft_real t0r = xr[0] + xr[2], t0i = xi[0] + xi[2];
        notorious_fft_real t1r = xr[0] - xr[2], t1i = xi[0] - xi[2];
        yr[0]      = t0r; yi[0]      = t0i;
        yr[2*sy]   = t1r; yi[2*sy]   = t1i;
        return;
    }
    if (N == 4) {
        notorious_fft_real t0r = xr[0] + xr[4], t0i = xi[0] + xi[4];
        notorious_fft_real t1r = xr[2] + xr[6], t1i = xi[2] + xi[6];
        notorious_fft_real t2r = xr[0] - xr[4], t2i = xi[0] - xi[4];
        notorious_fft_real t3r = -xi[2] + xi[6], t3i = xr[2] - xr[6];
        yr[0]      = t0r + t1r; yi[0]      = t0i + t1i;
        yr[2*sy]   = t2r + t3r; yi[2*sy]   = t2i + t3i;  /* swapped vs forward */
        yr[4*sy]   = t0r - t1r; yi[4*sy]   = t0i - t1i;
        yr[6*sy]   = t2r - t3r; yi[6*sy]   = t2i - t3i;  /* swapped vs forward */
        return;
    }
    if (N == 8) {
        notorious_fft_real t0r,t0i,t1r,t1i,t2r,t2i,t3r,t3i;
        notorious_fft_real t00r,t00i,t01r,t01i,t02r,t02i,t03r,t03i;
        notorious_fft_real t10r,t10i,t11r,t11i,t12r,t12i,t13r,t13i;
        notorious_fft_real ttr,tti;
        const notorious_fft_real invsqrt2 = NOTORIOUS_FFT_INV_SQRT2;

        t0r=xr[0]+xr[8];  t0i=xi[0]+xi[8];
        t1r=xr[4]+xr[12]; t1i=xi[4]+xi[12];
        t2r=xr[0]-xr[8];  t2i=xi[0]-xi[8];
        t3r=-xi[4]+xi[12]; t3i=xr[4]-xr[12];
        t00r=t0r+t1r; t00i=t0i+t1i;
        t01r=t2r+t3r; t01i=t2i+t3i;  /* swapped vs forward */
        t02r=t0r-t1r; t02i=t0i-t1i;
        t03r=t2r-t3r; t03i=t2i-t3i;  /* swapped vs forward */

        t0r=xr[2]+xr[10]; t0i=xi[2]+xi[10];
        t1r=xr[6]+xr[14]; t1i=xi[6]+xi[14];
        t2r=xr[2]-xr[10]; t2i=xi[2]-xi[10];
        t3r=-xi[6]+xi[14]; t3i=xr[6]-xr[14];

        t10r=t0r+t1r; t10i=t0i+t1i;
        /* t11=(t2+t3)*invsqrt2*(1+I) — conjugated vs forward */
        ttr=t2r+t3r; tti=t2i+t3i;
        t11r=invsqrt2*(ttr-tti); t11i=invsqrt2*(ttr+tti);
        /* t12=(t0-t1)*(-I) — same sign flip as forward but opposite */
        t12r=-t0i+t1i; t12i=t0r-t1r;
        /* t13=(t2-t3)*invsqrt2*(-1+I) — conjugated vs forward */
        ttr=t2r-t3r; tti=t2i-t3i;
        t13r=-invsqrt2*(ttr+tti); t13i=invsqrt2*(ttr-tti);

        yr[0]=t00r+t10r;    yi[0]=t00i+t10i;
        yr[2*sy]=t01r+t11r; yi[2*sy]=t01i+t11i;
        yr[4*sy]=t02r+t12r; yi[4*sy]=t02i+t12i;
        yr[6*sy]=t03r+t13r; yi[6*sy]=t03i+t13i;
        yr[8*sy]=t00r-t10r; yi[8*sy]=t00i-t10i;
        yr[10*sy]=t01r-t11r; yi[10*sy]=t01i-t11i;
        yr[12*sy]=t02r-t12r; yi[12*sy]=t02i-t12i;
        yr[14*sy]=t03r-t13r; yi[14*sy]=t03i-t13i;
        return;
    }

    /* General recursion: inverse split-radix DIF butterfly */
    /* N >= 16 */
    int n4 = N / 4;
    const notorious_fft_real* ep = e;

    {
        int k = 0;

#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
        for (; k + 2 <= n4; k += 2) {
            float64x2_t w1r = (float64x2_t){ep[4*k],   ep[4*(k+1)]};
            float64x2_t w1i = (float64x2_t){ep[4*k+1], ep[4*(k+1)+1]};
            float64x2_t w3r = (float64x2_t){ep[4*k+2], ep[4*(k+1)+2]};
            float64x2_t w3i = (float64x2_t){ep[4*k+3], ep[4*(k+1)+3]};

            float64x2x2_t xa = vld2q_f64(xr + 2*k);
            float64x2x2_t xb = vld2q_f64(xr + 2*(k+N/2));
            float64x2x2_t xc = vld2q_f64(xr + 2*(k+n4));
            float64x2x2_t xd = vld2q_f64(xr + 2*(k+3*n4));

            float64x2_t t0r_ = vaddq_f64(xa.val[0], xb.val[0]);
            float64x2_t t0i_ = vaddq_f64(xa.val[1], xb.val[1]);
            float64x2_t t1r_ = vaddq_f64(xc.val[0], xd.val[0]);
            float64x2_t t1i_ = vaddq_f64(xc.val[1], xd.val[1]);
            float64x2_t t2r_ = vsubq_f64(xa.val[0], xb.val[0]);
            float64x2_t t2i_ = vsubq_f64(xa.val[1], xb.val[1]);
            float64x2_t t3r_ = vsubq_f64(xd.val[1], xc.val[1]);
            float64x2_t t3i_ = vsubq_f64(xc.val[0], xd.val[0]);

            /* Inverse: u = t2+t3, v = t2-t3 (swapped vs forward) */
            float64x2_t ur_ = vaddq_f64(t2r_, t3r_), ui_ = vaddq_f64(t2i_, t3i_);
            float64x2_t vr_ = vsubq_f64(t2r_, t3r_), vi_ = vsubq_f64(t2i_, t3i_);

            float64x2x2_t out0 = {{t0r_, t0i_}};
            vst2q_f64(tr + 2*k, out0);
            float64x2x2_t out1 = {{t1r_, t1i_}};
            vst2q_f64(tr + 2*(k+n4), out1);

            /* conj(w1) multiply: re = ur*wr + ui*wi, im = -ur*wi + ui*wr */
            float64x2_t pr_ = vaddq_f64(vmulq_f64(ur_,w1r), vmulq_f64(ui_,w1i));
            float64x2_t pi_ = vsubq_f64(vmulq_f64(ui_,w1r), vmulq_f64(ur_,w1i));
            float64x2x2_t out2 = {{pr_, pi_}};
            vst2q_f64(tr + 2*(k+N/2), out2);

            /* conj(w3) multiply */
            float64x2_t qr_ = vaddq_f64(vmulq_f64(vr_,w3r), vmulq_f64(vi_,w3i));
            float64x2_t qi_ = vsubq_f64(vmulq_f64(vi_,w3r), vmulq_f64(vr_,w3i));
            float64x2x2_t out3 = {{qr_, qi_}};
            vst2q_f64(tr + 2*(k+3*n4), out3);
        }
#endif

        /* Scalar remainder / fallback */
        for (; k < n4; k++) {
            notorious_fft_real x0r=xr[2*k],x0i=xi[2*k];
            notorious_fft_real x1r=xr[2*(k+N/2)],x1i=xi[2*(k+N/2)];
            notorious_fft_real x2r=xr[2*(k+n4)],x2i=xi[2*(k+n4)];
            notorious_fft_real x3r=xr[2*(k+3*n4)],x3i=xi[2*(k+3*n4)];
            notorious_fft_real t0r_=x0r+x1r,t0i_=x0i+x1i;
            notorious_fft_real t1r_=x2r+x3r,t1i_=x2i+x3i;
            notorious_fft_real t2r_=x0r-x1r,t2i_=x0i-x1i;
            notorious_fft_real t3r_=x3i-x2i,t3i_=x2r-x3r;
            /* Inverse: u=t2+t3, v=t2-t3 (swapped vs forward) */
            notorious_fft_real ur_=t2r_+t3r_,ui_=t2i_+t3i_;
            notorious_fft_real vr_=t2r_-t3r_,vi_=t2i_-t3i_;
            tr[2*k]=t0r_; ti[2*k]=t0i_;
            tr[2*(k+n4)]=t1r_; ti[2*(k+n4)]=t1i_;
            /* conj(e) multiply: re = ur*er + ui*ei, im = -ur*ei + ui*er */
            tr[2*(k+N/2)]=ur_*ep[4*k]+ui_*ep[4*k+1];
            ti[2*(k+N/2)]=-ur_*ep[4*k+1]+ui_*ep[4*k];
            tr[2*(k+3*n4)]=vr_*ep[4*k+2]+vi_*ep[4*k+3];
            ti[2*(k+3*n4)]=-vr_*ep[4*k+3]+vi_*ep[4*k+2];
        }
    }

    const notorious_fft_real* e_next = e + N;
    notorious_fft_sr_inv_dif_cx(N/2, t,          t,          y,       2*sy, e_next);
    notorious_fft_sr_inv_dif_cx(N/4, t+N,        t+N,        y+2*sy,  4*sy, e_next + N/2);
    notorious_fft_sr_inv_dif_cx(N/4, t+3*(N/2),  t+3*(N/2),  y+6*sy,  4*sy, e_next + N/2);
}

/* Public wrapper: run split-radix DIF on interleaved complex data */
static void notorious_fft_execute_sr_dif(
    const notorious_fft_plan* plan,
    const notorious_fft_real* x_in,
    notorious_fft_real* y_out,
    int inverse)
{
    int N = (int)plan->n;
    notorious_fft_real* t = plan->sr_t;  /* temp buffer: 2*N reals */

    /* Copy input to temp (sr_dif writes to t during butterfly stage) */
    memcpy(t, x_in, (size_t)(2*N) * sizeof(notorious_fft_real));
    if (inverse) {
        notorious_fft_sr_inv_dif_cx(N, t, plan->work_re, y_out, 1, plan->sr_e);
    } else {
        notorious_fft_sr_dif_cx(N, t, plan->work_re, y_out, 1, plan->sr_e);
    }
}

/* ============================================================================
 * Bluestein's Algorithm for Arbitrary Size FFT
 * ============================================================================ */

static void notorious_fft_execute_bluestein(
    const notorious_fft_plan* plan,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_in, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_in,
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT xr_out, notorious_fft_real* NOTORIOUS_FFT_RESTRICT xi_out)
{
    if (!plan || !plan->bluestein_plan ||
        !plan->bluestein_chirp_re || !plan->bluestein_chirp_im ||
        !plan->bluestein_buf_re || !plan->bluestein_buf_im) {
        return;
    }

    size_t n = plan->n;
    size_t m = plan->bluestein_n;
    
    /* For inverse DFT, use the property: IDFT(X) = (1/N) * conj(DFT(conj(X)))
     * This lets us use the same forward Bluestein algorithm for inverse. */
    if (plan->is_inverse) {
        /* Use bluestein buffers for temporary storage */
        notorious_fft_real* x_conj_re = plan->bluestein_buf_re;
        notorious_fft_real* x_conj_im = plan->bluestein_buf_im;
        
        /* Compute conj(X) */
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n > 1024)
#endif
        for (size_t i = 0; i < n; i++) {
            x_conj_re[i] = xr_in[i];
            x_conj_im[i] = -xi_in[i];
        }
        
        /* Apply forward Bluestein to conj(X) */
        /* First save original is_inverse flag and set to forward */
        int saved_inverse = plan->is_inverse;
        ((notorious_fft_plan*)plan)->is_inverse = 0;
        
        notorious_fft_execute_bluestein(plan, x_conj_re, x_conj_im, xr_out, xi_out);
        
        /* Restore inverse flag */
        ((notorious_fft_plan*)plan)->is_inverse = saved_inverse;
        
        /* Take conj(y) and scale by 1/N: IDFT(X) = conj(y) / N */
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n > 1024)
#endif
        for (size_t i = 0; i < n; i++) {
            notorious_fft_real tmp_re = xr_out[i] / (notorious_fft_real)n;
            notorious_fft_real tmp_im = -xi_out[i] / (notorious_fft_real)n;
            xr_out[i] = tmp_re;
            xi_out[i] = tmp_im;
        }
        
        return;
    }
    
    /* Forward Bluestein algorithm */
    
    /* chirp_re/im contains the original chirp factors exp(-i*pi*k^2/n) for pre/post multiply */
    const notorious_fft_real* chirp_re = plan->bluestein_chirp_re;
    const notorious_fft_real* chirp_im = plan->bluestein_chirp_im;
    /* chirp_fft_re/im contains conj(FFT(chirp)) for frequency-domain convolution */
    const notorious_fft_real* chirp_fft_re = plan->bluestein_chirp_fft_re;
    const notorious_fft_real* chirp_fft_im = plan->bluestein_chirp_fft_im;
    notorious_fft_real* buf_re = plan->bluestein_buf_re;
    notorious_fft_real* buf_im = plan->bluestein_buf_im;
    
    /* Step 1: Multiply input by chirp: x[n] * exp(-i*pi*n^2/N) */
#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
    if (n >= 4) {
        notorious_fft_bluestein_premul_avx2(n, buf_re, buf_im, xr_in, xi_in, chirp_re, chirp_im);
        notorious_fft_bluestein_zeropad_avx2(n, m, buf_re, buf_im);
    } else
#elif NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
    if (n >= 2) {
        notorious_fft_bluestein_premul_neon(n, buf_re, buf_im, xr_in, xi_in, chirp_re, chirp_im);
        notorious_fft_bluestein_zeropad_neon(n, m, buf_re, buf_im);
    } else
#endif
    {
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n > 1024)
#endif
        for (size_t i = 0; i < n; i++) {
            buf_re[i] = xr_in[i] * chirp_re[i] - xi_in[i] * chirp_im[i];
            buf_im[i] = xr_in[i] * chirp_im[i] + xi_in[i] * chirp_re[i];
        }
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(m - n > 1024)
#endif
        for (size_t i = n; i < m; i++) {
            buf_re[i] = 0;
            buf_im[i] = 0;
        }
    }
    
    /* Step 2: FFT of padded sequence
     * Use separate fft_buf for output since iterative FFT clobbers work buffers */
    notorious_fft_real* fft_re = plan->bluestein_fft_buf_re;
    notorious_fft_real* fft_im = plan->bluestein_fft_buf_im;
    notorious_fft_execute_iterative_internal(plan->bluestein_plan, buf_re, buf_im, fft_re, fft_im, 0);
    
    /* Step 3: Pointwise multiply with conj(FFT(chirp)) for convolution */
#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
    if (m >= 4) {
        notorious_fft_bluestein_convolve_avx2(m, fft_re, fft_im, chirp_fft_re, chirp_fft_im);
    } else
#elif NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
    if (m >= 2) {
        notorious_fft_bluestein_convolve_neon(m, fft_re, fft_im, chirp_fft_re, chirp_fft_im);
    } else
#endif
    {
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(m > 1024)
#endif
        for (size_t i = 0; i < m; i++) {
            notorious_fft_real tr = fft_re[i] * chirp_fft_re[i] - fft_im[i] * chirp_fft_im[i];
            notorious_fft_real ti = fft_re[i] * chirp_fft_im[i] + fft_im[i] * chirp_fft_re[i];
            fft_re[i] = tr;
            fft_im[i] = ti;
        }
    }
    
    /* Step 4: IFFT using inverse FFT */
    notorious_fft_execute_iterative_internal(plan->bluestein_plan, fft_re, fft_im, buf_re, buf_im, 1);
    
    /* Step 5: Scale and multiply by original chirp to get final result
     * 
     * The convolution result is at indices [n-1, 2n-2] of the IFFT output.
     * We take the first n values from there, multiply by chirp, and scale.
     * Scale by 1/m from the IFFT (unscaled iterative IFFT).
     */
    notorious_fft_real scale = 1.0 / (notorious_fft_real)m;
    size_t conv_offset = n - 1;  /* Start of valid convolution results */
    
#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
    if (n >= 2) {
        /* Use scalar loop with offset since NEON version doesn't support offset */
        for (size_t i = 0; i < n; i++) {
            size_t j = conv_offset + i;
            xr_out[i] = (buf_re[j] * chirp_re[i] - buf_im[j] * chirp_im[i]) * scale;
            xi_out[i] = (buf_re[j] * chirp_im[i] + buf_im[j] * chirp_re[i]) * scale;
        }
    } else
#endif
    {
#if NOTORIOUS_FFT_HAS_OPENMP
        #pragma omp parallel for schedule(static) if(n > 1024)
#endif
        for (size_t i = 0; i < n; i++) {
            size_t j = conv_offset + i;
            xr_out[i] = (buf_re[j] * chirp_re[i] - buf_im[j] * chirp_im[i]) * scale;
            xi_out[i] = (buf_re[j] * chirp_im[i] + buf_im[j] * chirp_re[i]) * scale;
        }
    }
}

/* ==========================================================================
 * 06_plan.h
 * ========================================================================== */

/*
 * Notorious FFT - Plan Creation with Runtime Algorithm Selection
 */



/* Forward declarations */
static notorious_fft_plan* notorious_fft_create_plan_power2(size_t n);
static void notorious_fft_destroy_plan(notorious_fft_plan* plan);

/* ============================================================================
 * Slab sizing helpers
 *
 * Every field that will be bump-allocated is rounded up to NOTORIOUS_FFT_BUMP_ALIGN
 * bytes.  We define a single macro so the accounting in the "compute total"
 * and the "bump alloc" steps stay in sync.
 * ============================================================================ */

#define NOTORIOUS_FFT_SLAB_FIELD(bytes) NOTORIOUS_FFT_BUMP_ROUND(bytes)

/* ============================================================================
 * Bluestein Plan Creation
 * ============================================================================ */

static notorious_fft_plan* notorious_fft_create_plan_bluestein(size_t n, int inverse) {
    if (n == 0) return NULL;

    /* Next power of 2 >= 2*n-1 */
    size_t m = 1;
    while (m < 2 * n - 1) m <<= 1;

    /* Slab layout (high→low, allocated by decrementing bump pointer):
     *
     *   [notorious_fft_plan struct]           ← slab base (low address)
     *   ~~~~ padding to 64 bytes ~~~~
     *   bluestein_chirp_re  [m reals]
     *   bluestein_chirp_im  [m reals]
     *   bluestein_chirp_fft_re [m reals]
     *   bluestein_chirp_fft_im [m reals]
     *   bluestein_buf_re    [m reals]
     *   bluestein_buf_im    [m reals]
     *   bluestein_fft_buf_re [m reals]
     *   bluestein_fft_buf_im [m reals]
     *   work_re             [m reals]
     *   work_im             [m reals]   ← slab end (high address, bump starts here)
     */
    size_t real_bytes = m * sizeof(notorious_fft_real);
    size_t total = NOTORIOUS_FFT_SLAB_FIELD(sizeof(notorious_fft_plan))
                 + NOTORIOUS_FFT_SLAB_FIELD(real_bytes) * 10;  /* 10 arrays of m reals */
    /* Round total to alignment - ensures bump starts at aligned address */
    total = NOTORIOUS_FFT_BUMP_ROUND(total);

    void* slab = notorious_fft_malloc(total);
    if (!slab) return NULL;

    /* Plan lives at the base of the slab */
    notorious_fft_plan* plan = (notorious_fft_plan*)slab;
    memset(plan, 0, sizeof(notorious_fft_plan));

    plan->slab         = slab;
    plan->n            = n;
    plan->is_inverse   = inverse;
    plan->execute_func = notorious_fft_execute_bluestein;
    plan->bluestein_n  = m;

    /* Bump pointer starts at the end of the slab and decrements downward */
    char* bump = (char*)slab + total;

    plan->work_im              = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->work_re              = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_fft_buf_im = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_fft_buf_re = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_buf_im     = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_buf_re     = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_chirp_fft_im = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_chirp_fft_re = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_chirp_im   = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    plan->bluestein_chirp_re   = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, real_bytes);
    /* bump now points just above sizeof(notorious_fft_plan) — slab is fully packed */

    /* Inner power-of-2 plan (owns its own slab, freed separately) */
    plan->bluestein_plan = notorious_fft_create_plan_power2(m);
    if (!plan->bluestein_plan) {
        goto cleanup;
    }

    /* Create filter h[n] = exp(πin²/N) for n = -(N-1)..(N-1) in work buffers */
    for (size_t k = 0; k <= 2*(n-1); k++) {
        int idx = (int)k - (int)(n - 1);
        notorious_fft_real angle = NOTORIOUS_FFT_PI * (notorious_fft_real)idx * (notorious_fft_real)idx / (notorious_fft_real)n;
        plan->work_re[k] = notorious_fft_cos(angle);
        plan->work_im[k] = notorious_fft_sin(angle);
    }
    for (size_t k = 2*n-1; k < m; k++) {
        plan->work_re[k] = 0;
        plan->work_im[k] = 0;
    }

    /* FFT the filter h → bluestein_chirp_fft */
    notorious_fft_execute_iterative(plan->bluestein_plan,
                             plan->work_re, plan->work_im,
                             plan->bluestein_chirp_fft_re, plan->bluestein_chirp_fft_im);

    /* Chirp a[n] = exp(-πin²/N) for n = 0..N-1 */
    for (size_t i = 0; i < n; i++) {
        notorious_fft_real angle = -NOTORIOUS_FFT_PI * (notorious_fft_real)i * (notorious_fft_real)i / (notorious_fft_real)n;
        plan->bluestein_chirp_re[i] = notorious_fft_cos(angle);
        plan->bluestein_chirp_im[i] = notorious_fft_sin(angle);
    }

    return plan;

cleanup:
    if (plan->bluestein_plan)
        notorious_fft_destroy_plan(plan->bluestein_plan);
    notorious_fft_free(slab);
    return NULL;
}

/* ============================================================================
 * Power-of-2 Plan Creation
 * ============================================================================ */

static notorious_fft_plan* notorious_fft_create_plan_power2(size_t n) {
    if (n == 0 || (n & (n - 1))) return NULL;

    size_t real_bytes  = sizeof(notorious_fft_real);
    size_t int_bytes   = sizeof(int);

    size_t total;

    if (n <= NOTORIOUS_FFT_SMALL_SIZE) {
        /* Small plan slab layout (high→low):
         *   [notorious_fft_plan]
         *   work_im    [n reals]
         *   work_re    [n reals]
         *   tw_im      [n/2+1 reals]
         *   tw_re      [n/2+1 reals]
         *   bitrev     [n ints]
         */
        total = NOTORIOUS_FFT_SLAB_FIELD(sizeof(notorious_fft_plan))
              + NOTORIOUS_FFT_SLAB_FIELD(n * int_bytes)
              + NOTORIOUS_FFT_SLAB_FIELD((n / 2 + 1) * real_bytes) * 2
              + NOTORIOUS_FFT_SLAB_FIELD(n * real_bytes) * 2;
    } else {
        /* Round total to alignment - ensures bump starts at aligned address */
        /* Large plan slab layout (high→low):
         *   [notorious_fft_plan]
         *   sr_t       [2n reals]
         *   sr_e       [2n reals]
         *   work_re    [2n reals]  ← work_im = work_re + n (no extra field)
         *   tw_im      [n/2 reals]
         *   tw_re      [n/2 reals]
         *   bitrev     [n ints]
         */
        total = NOTORIOUS_FFT_SLAB_FIELD(sizeof(notorious_fft_plan))
              + NOTORIOUS_FFT_SLAB_FIELD(n * int_bytes)
              + NOTORIOUS_FFT_SLAB_FIELD((n / 2) * real_bytes) * 2
              + NOTORIOUS_FFT_SLAB_FIELD(2 * n * real_bytes)     /* work_re (2n, work_im aliased) */
              + NOTORIOUS_FFT_SLAB_FIELD(2 * n * real_bytes) * 2;/* sr_e, sr_t */
        total = NOTORIOUS_FFT_BUMP_ROUND(total);
    }

    void* slab = notorious_fft_malloc(total);
    if (!slab) return NULL;

    notorious_fft_plan* plan = (notorious_fft_plan*)slab;
    memset(plan, 0, sizeof(notorious_fft_plan));

    plan->slab          = slab;
    plan->n             = n;

    char* bump = (char*)slab + total;

    if (n <= NOTORIOUS_FFT_SMALL_SIZE) {
        plan->work_im  = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, n * real_bytes);
        plan->work_re  = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, n * real_bytes);
        plan->tw_im    = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, (n / 2 + 1) * real_bytes);
        plan->tw_re    = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, (n / 2 + 1) * real_bytes);
        plan->bitrev   = (int*)        notorious_fft_bump_alloc(&bump, n * int_bytes);

        plan->execute_func = notorious_fft_execute_iterative;

        notorious_fft_compute_bitrev(plan->bitrev, n);
        for (size_t i = 0; i < n / 2; i++) {
            notorious_fft_real angle = -NOTORIOUS_FFT_2PI * (notorious_fft_real)i / (notorious_fft_real)n;
            plan->tw_re[i] = notorious_fft_cos(angle);
            plan->tw_im[i] = notorious_fft_sin(angle);
        }
    } else {
        plan->sr_t     = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, 2 * n * real_bytes);
        plan->sr_e     = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, 2 * n * real_bytes);
        plan->work_re  = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, 2 * n * real_bytes);
        plan->work_im  = plan->work_re + n;  /* alias into same block */
        plan->tw_im    = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, (n / 2) * real_bytes);
        plan->tw_re    = (notorious_fft_real*)notorious_fft_bump_alloc(&bump, (n / 2) * real_bytes);
        plan->bitrev   = (int*)        notorious_fft_bump_alloc(&bump, n * int_bytes);

        plan->execute_func = notorious_fft_execute_iterative;

        notorious_fft_compute_bitrev(plan->bitrev, n);
        for (size_t i = 0; i < n / 2; i++) {
            notorious_fft_real angle = -NOTORIOUS_FFT_2PI * (notorious_fft_real)i / (notorious_fft_real)n;
            plan->tw_re[i] = notorious_fft_cos(angle);
            plan->tw_im[i] = notorious_fft_sin(angle);
        }

        /* Split-radix DIF twiddles */
        notorious_fft_real* ep = plan->sr_e;
        size_t sz = n;
        while (sz >= 16) {
            for (size_t k = 0; k < sz / 4; k++) {
                *ep++ = notorious_fft_cos(-NOTORIOUS_FFT_2PI * (notorious_fft_real)k / (notorious_fft_real)sz);
                *ep++ = notorious_fft_sin(-NOTORIOUS_FFT_2PI * (notorious_fft_real)k / (notorious_fft_real)sz);
                *ep++ = notorious_fft_cos(-NOTORIOUS_FFT_2PI * 3.0 * (notorious_fft_real)k / (notorious_fft_real)sz);
                *ep++ = notorious_fft_sin(-NOTORIOUS_FFT_2PI * 3.0 * (notorious_fft_real)k / (notorious_fft_real)sz);
            }
            sz >>= 1;
        }
    }

    return plan;
}

/* ============================================================================
 * Main Plan API
 * ============================================================================ */

static notorious_fft_plan* notorious_fft_create_plan(size_t n, int inverse) {
    if (n == 0) return NULL;
    if ((n & (n - 1)) == 0)
        return notorious_fft_create_plan_power2(n);
    return notorious_fft_create_plan_bluestein(n, inverse);
}

static void notorious_fft_destroy_plan(notorious_fft_plan* plan) {
    if (!plan) return;
    /* Bluestein inner plan owns its own slab */
    if (plan->bluestein_plan)
        notorious_fft_destroy_plan(plan->bluestein_plan);
    /* Everything else is in the single slab — one free to rule them all */
    notorious_fft_free(plan->slab);
}

/* ==========================================================================
 * 07_legacy.h
 * ========================================================================== */

/*
 * Notorious FFT - Legacy API
 *
 * All twiddles precomputed at plan time,
 * all scratch buffers preallocated — zero malloc at transform time.
 */



/* ============================================================================
 * Complex Number Type for API Compatibility
 * ============================================================================ */

/* Interleaved (re, im) pair
 * In C++ we use a plain struct so no C99 _Complex dependency is needed. */
#ifdef __cplusplus
    typedef struct { notorious_fft_real re, im; } notorious_fft_cmpl;
#else
    typedef notorious_fft_real notorious_fft_cmpl[2];
#endif

/* ============================================================================
 * Transform Function Prototypes
 * ============================================================================ */

void notorious_fft_dft(notorious_fft_cmpl* x, notorious_fft_cmpl* y, const notorious_fft_aux* a);
void notorious_fft_invdft(notorious_fft_cmpl* x, notorious_fft_cmpl* y, const notorious_fft_aux* a);
void notorious_fft_realdft(notorious_fft_real* x, notorious_fft_cmpl* z, const notorious_fft_aux* a);
void notorious_fft_invrealdft(notorious_fft_cmpl* z, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dct2(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dst2(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dct3(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dst3(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dct4(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);
void notorious_fft_dst4(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a);

notorious_fft_aux* notorious_fft_mkaux_dft_1d(int N);
notorious_fft_aux* notorious_fft_mkaux_dft_2d(int N1, int N2);
notorious_fft_aux* notorious_fft_mkaux_dft_3d(int N1, int N2, int N3);
notorious_fft_aux* notorious_fft_mkaux_dft(int d, int* Ns);
notorious_fft_aux* notorious_fft_mkaux_realdft_1d(int N);
notorious_fft_aux* notorious_fft_mkaux_realdft_2d(int N1, int N2);
notorious_fft_aux* notorious_fft_mkaux_realdft_3d(int N1, int N2, int N3);
notorious_fft_aux* notorious_fft_mkaux_realdft(int d, int* Ns);
notorious_fft_aux* notorious_fft_mkaux_t2t3_1d(int N);
notorious_fft_aux* notorious_fft_mkaux_t2t3_2d(int N1, int N2);
notorious_fft_aux* notorious_fft_mkaux_t2t3_3d(int N1, int N2, int N3);
notorious_fft_aux* notorious_fft_mkaux_t2t3(int d, int* Ns);
notorious_fft_aux* notorious_fft_mkaux_t4_1d(int N);
notorious_fft_aux* notorious_fft_mkaux_t4_2d(int N1, int N2);
notorious_fft_aux* notorious_fft_mkaux_t4_3d(int N1, int N2, int N3);
notorious_fft_aux* notorious_fft_mkaux_t4(int d, int* Ns);

void notorious_fft_free_aux(notorious_fft_aux* a);

/* ============================================================================
 * Implementation
 * ============================================================================ */

#ifdef NOTORIOUS_FFT_IMPLEMENTATION

/* C++ compatible cast */
#ifdef __cplusplus
    #define NOTORIOUS_FFT_CAST(T, x) static_cast<T>(x)
#else
    #define NOTORIOUS_FFT_CAST(T, x) (x)
#endif

/* 1D DFT operating directly on interleaved complex — no deinterleave round-trip */
static void notorious_fft_dft_1d_cx(notorious_fft_cmpl* x, notorious_fft_cmpl* y, int N,
                              const notorious_fft_plan* plan, int inverse) {
    notorious_fft_execute_cx(plan, (notorious_fft_real*)x, (notorious_fft_real*)y, inverse);
}

/* Strided 1D DFT: gather → contiguous FFT → scatter.
 * Matches minfft's s_dft_1d(x, y, sy, a) interface.
 * All pointers are in units of notorious_fft_cmpl (2 reals). */
static void notorious_fft_s_dft_1d(notorious_fft_cmpl* x, notorious_fft_cmpl* y, int sy,
                              const notorious_fft_aux* a, int inverse) {
    int N = a->N;
    notorious_fft_plan* plan = a->plan;
    if (!plan) return;

    if (sy == 1) {
        notorious_fft_dft_1d_cx(x, y, N, plan, inverse);
    } else {
        notorious_fft_cmpl* buf = (notorious_fft_cmpl*)malloc((size_t)N * sizeof(notorious_fft_cmpl));
        if (!buf) return;
        notorious_fft_dft_1d_cx(x, buf, N, plan, inverse);
        notorious_fft_real* br = (notorious_fft_real*)buf;
        notorious_fft_real* yr = (notorious_fft_real*)y;
        for (int i = 0; i < N; i++) {
            yr[2 * i * sy]     = br[2 * i];
            yr[2 * i * sy + 1] = br[2 * i + 1];
        }
        free(buf);
    }
}

/* Recursive strided multi-dimensional DFT — mirrors minfft's mkcx().
 *
 * Aux tree structure (from notorious_fft_make_aux):
 *   d==1 wrapper: plan=NULL, sub1=NULL, sub2=1D_aux (has plan)
 *   d>1 node:     plan=NULL, sub1=make_aux(d-1), sub2=1D_aux (has plan), t=temp buf
 *   direct 1D:    plan!=NULL (from mkaux_dft_1d called directly) */
static void notorious_fft_mkcx(notorious_fft_cmpl* x, notorious_fft_cmpl* y, int sy,
                          const notorious_fft_aux* a, int inverse) {
    /* Direct 1D aux (plan set, no sub-structures) */
    if (a->plan) {
        notorious_fft_s_dft_1d(x, y, sy, a, inverse);
        return;
    }

    if (a->sub1 == NULL) {
        /* d==1 wrapper from make_aux: sub2 is the actual 1D aux */
        if (a->sub2)
            notorious_fft_s_dft_1d(x, y, sy, a->sub2, inverse);
        return;
    }

    /* Recursive case: d > 1
     * sub1 = (d-1)-dimensional aux for inner dimensions
     * sub2 = 1D aux for outermost dimension
     * N1 = product of inner dims, N2 = outermost dim size */
    int N1 = a->sub1->N;
    int N2 = a->sub2->N;
    notorious_fft_cmpl* t = (notorious_fft_cmpl*)a->t;

    /* Pass 1: transform each hyperplane (inner dims), writing transposed into t */
    /* No OpenMP — shared plan buffers (sr_t, work_re, t) are not thread-safe */
    for (int n = 0; n < N2; n++)
        notorious_fft_mkcx(x + n * N1, t + n, N2, a->sub1, inverse);

    /* Pass 2: transform outermost dimension (now contiguous rows in t) */
    for (int n = 0; n < N1; n++)
        notorious_fft_s_dft_1d(t + n * N2, y + sy * n, sy * N1, a->sub2, inverse);
}

/* ============================================================================
 * Complex DFT
 * ============================================================================ */

void notorious_fft_dft(notorious_fft_cmpl* x, notorious_fft_cmpl* y, const notorious_fft_aux* a) {
    notorious_fft_mkcx(x, y, 1, a, 0);
}

void notorious_fft_invdft(notorious_fft_cmpl* x, notorious_fft_cmpl* y, const notorious_fft_aux* a) {
    notorious_fft_mkcx(x, y, 1, a, 1);
}

/* ============================================================================
 * Real DFT — Precomputed twiddles, split-radix DIF inner FFT
 *
 * aux->e  = precomputed unpack twiddles: pairs (tw_r, tw_i) for k=0..N/4-1
 *           where tw_r = cos(-2πk/N), tw_i = sin(-2πk/N)
 * aux->t  = preallocated scratch buffer (N reals = N/2 interleaved complex)
 * ============================================================================ */

void notorious_fft_realdft(notorious_fft_real* x, notorious_fft_cmpl* z, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) {
        ((notorious_fft_real*)z)[0] = x[0];
        ((notorious_fft_real*)z)[1] = 0;
        return;
    }
    if (N == 2) {
        notorious_fft_real t0 = x[0] + x[1];
        notorious_fft_real t1 = x[0] - x[1];
        ((notorious_fft_real*)z)[0] = t0;
        ((notorious_fft_real*)z)[1] = 0;
        ((notorious_fft_real*)z)[2] = t1;
        ((notorious_fft_real*)z)[3] = 0;
        return;
    }

    int M = N / 2;
    notorious_fft_real* zr = (notorious_fft_real*)z;

    /* Use scratch buffer for the N/2-point complex FFT.
     * We need separate input/output because notorious_fft_execute_cx may not be in-place. */
    notorious_fft_real* scratch = (notorious_fft_real*)a->t;  /* N reals = M interleaved complex */

    /* Pack real input as interleaved complex: {x[0],x[1]}, {x[2],x[3]}, ... */
    memcpy(scratch, x, (size_t)N * sizeof(notorious_fft_real));

    /* N/2-point complex FFT via split-radix DIF */
    notorious_fft_execute_cx(a->plan, scratch, zr, 0 /* forward */);

    /* Unpack using precomputed twiddles from aux->e */
    const notorious_fft_real* tw = (const notorious_fft_real*)a->e;  /* pairs: (cos, sin) for k=0..N/4-1 */

    /* Save z[0] before overwriting */
    notorious_fft_real t0r = zr[0], t0i = zr[1];

    /* DC (k=0) */
    zr[0] = t0r + t0i;
    zr[1] = 0;

    /* Nyquist (k=N/2) */
    zr[2*M] = t0r - t0i;
    zr[2*M+1] = 0;

    /* Bins k = 1 to N/4 - 1 — precomputed twiddles, no trig calls */
    for (int k = 1; k < N/4; k++) {
        notorious_fft_real tkr = zr[2*k], tki = zr[2*k+1];
        notorious_fft_real t_n_2_k_r = zr[2*(M-k)], t_n_2_k_i = zr[2*(M-k)+1];

        notorious_fft_real ur = (tkr + t_n_2_k_r) * (notorious_fft_real)0.5;
        notorious_fft_real ui = (tki - t_n_2_k_i) * (notorious_fft_real)0.5;

        notorious_fft_real diff_r = tkr - t_n_2_k_r;
        notorious_fft_real diff_i = tki + t_n_2_k_i;

        notorious_fft_real tw_r = tw[2*k];
        notorious_fft_real tw_i = tw[2*k+1];

        notorious_fft_real prod_r = diff_r * tw_r - diff_i * tw_i;
        notorious_fft_real prod_i = diff_r * tw_i + diff_i * tw_r;

        notorious_fft_real vr = prod_i * (notorious_fft_real)0.5;
        notorious_fft_real vi = -prod_r * (notorious_fft_real)0.5;

        /* Write high bins first to avoid clobbering low bins */
        zr[2*(M-k)]   = ur - vr;
        zr[2*(M-k)+1] = -(ui - vi);
        zr[2*k]   = ur + vr;
        zr[2*k+1] = ui + vi;
    }

    /* z[N/4]: imaginary negated (conjugate of t[N/4]) */
    if (N >= 4) {
        zr[2*(N/4)+1] = -zr[2*(N/4)+1];
    }
}

void notorious_fft_invrealdft(notorious_fft_cmpl* z, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) {
        y[0] = ((notorious_fft_real*)z)[0];
        return;
    }
    if (N == 2) {
        notorious_fft_real t0 = ((notorious_fft_real*)z)[0];
        notorious_fft_real t1 = ((notorious_fft_real*)z)[2];
        y[0] = t0 + t1;
        y[1] = t0 - t1;
        return;
    }

    int M = N / 2;
    /* Use preallocated scratch buffer — no malloc */
    notorious_fft_real* ibuf = (notorious_fft_real*)a->t;  /* N reals = M interleaved complex */

    notorious_fft_real* zr = (notorious_fft_real*)z;

    /* Precomputed twiddles — same table as forward, but conjugated usage */
    const notorious_fft_real* tw = (const notorious_fft_real*)a->e;

    /* Unpack spectrum z[0..M] → interleaved complex for N/2-point IFFT */
    ibuf[0] = zr[0] + zr[2*M];
    ibuf[1] = zr[0] - zr[2*M];

    for (int k = 1; k < N/4; k++) {
        notorious_fft_real z_k_r = zr[2*k],     z_k_i = zr[2*k+1];
        notorious_fft_real z_q_r = zr[2*(M-k)], z_q_i = -zr[2*(M-k)+1];

        notorious_fft_real ur = z_k_r + z_q_r, ui = z_k_i + z_q_i;
        notorious_fft_real dr = z_k_r - z_q_r, di = z_k_i - z_q_i;

        /* Use precomputed twiddle (negated angle = conjugate) */
        notorious_fft_real ce_r = tw[2*k];    /* cos(-2πk/N) */
        notorious_fft_real ce_i = -tw[2*k+1]; /* -sin(-2πk/N) = sin(2πk/N) */

        notorious_fft_real vr = -di * ce_r - dr * ce_i;
        notorious_fft_real vi = -di * ce_i + dr * ce_r;

        ibuf[2*k]       = ur + vr;  ibuf[2*k+1]       = ui + vi;
        ibuf[2*(M-k)]   = ur - vr;  ibuf[2*(M-k)+1]   = -(ui - vi);
    }

    if (N >= 4) {
        ibuf[2*(N/4)]   =  2 * zr[2*(N/4)];
        ibuf[2*(N/4)+1] = -2 * zr[2*(N/4)+1];
    }

    /* Inverse N/2-point complex FFT via split-radix DIF */
    notorious_fft_execute_cx(a->plan, ibuf, y, 1 /* inverse */);

    /* Output y is already the real data (interleaved complex → real pairs) */
}

/* ============================================================================
 * NEON DCT-2 Acceleration (Double Precision)
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)

/* Hardcoded N=8 DCT-2 using NEON - fully unrolled */
static void notorious_fft_dct2_neon_n8(const notorious_fft_real* NOTORIOUS_FFT_RESTRICT x,
                                  notorious_fft_real* NOTORIOUS_FFT_RESTRICT y,
                                  const notorious_fft_real* NOTORIOUS_FFT_RESTRICT dct_tw,
                                  notorious_fft_real* NOTORIOUS_FFT_RESTRICT reordered,
                                  notorious_fft_real* NOTORIOUS_FFT_RESTRICT z_buf) {
    /* Reorder: even indices first, then odd reversed */
    float64x2_t x0 = vld1q_f64(x + 0);  /* x[0], x[1] */
    float64x2_t x2 = vld1q_f64(x + 2);  /* x[2], x[3] */
    float64x2_t x4 = vld1q_f64(x + 4);  /* x[4], x[5] */
    float64x2_t x6 = vld1q_f64(x + 6);  /* x[6], x[7] */
    
    /* Extract even indices: x[0], x[2], x[4], x[6] */
    reordered[0] = vgetq_lane_f64(x0, 0);
    reordered[1] = vgetq_lane_f64(x2, 0);
    reordered[2] = vgetq_lane_f64(x4, 0);
    reordered[3] = vgetq_lane_f64(x6, 0);
    
    /* Odd reversed: x[7], x[5], x[3], x[1] */
    reordered[4] = vgetq_lane_f64(x6, 1);
    reordered[5] = vgetq_lane_f64(x4, 1);
    reordered[6] = vgetq_lane_f64(x2, 1);
    reordered[7] = vgetq_lane_f64(x0, 1);
    
    /* Real DFT on reordered - 4-point complex FFT */
    float64x2_t a0 = vld1q_f64(reordered + 0);
    float64x2_t a1 = vld1q_f64(reordered + 2);
    float64x2_t a2 = vld1q_f64(reordered + 4);
    float64x2_t a3 = vld1q_f64(reordered + 6);
    
    float64x2_t b0 = vaddq_f64(a0, a2);
    float64x2_t b1 = vaddq_f64(a1, a3);
    float64x2_t b2 = vsubq_f64(a0, a2);
    float64x2_t b3 = vsubq_f64(a1, a3);
    
    vst1q_f64(z_buf + 0, b0);
    vst1q_f64(z_buf + 2, b1);
    vst1q_f64(z_buf + 4, b2);
    vst1q_f64(z_buf + 6, b3);
    
    /* Post-process with DCT twiddles using NEON */
    y[0] = 2.0 * z_buf[0];
    
    /* Process k=1,2 using NEON - 2 at a time */
    float64x2_t cos_12 = {dct_tw[2], dct_tw[4]};
    float64x2_t sin_12 = {dct_tw[3], dct_tw[5]};
    
    float64x2_t z_1 = vld1q_f64(z_buf + 2);  /* zr[1], zi[1] */
    float64x2_t z_2 = vld1q_f64(z_buf + 4);  /* zr[2], zi[2] */
    
    float64x2_t zr_12 = vuzp1q_f64(z_1, z_2);  /* zr[1], zr[2] */
    float64x2_t zi_12 = vuzp2q_f64(z_1, z_2);  /* zi[1], zi[2] */
    
    /* y[k] = 2 * (zr_k * cos_k - zi_k * sin_k) */
    float64x2_t yk = vmulq_f64(zr_12, cos_12);
    yk = vfmsq_f64(yk, zi_12, sin_12);
    yk = vmulq_n_f64(yk, 2.0);
    
    /* y[N-k] = -2 * (zr_k * sin_k + zi_k * cos_k) */
    float64x2_t yNk = vmulq_f64(zr_12, sin_12);
    yNk = vfmaq_f64(yNk, zi_12, cos_12);
    yNk = vmulq_n_f64(yNk, -2.0);
    
    y[1] = vgetq_lane_f64(yk, 0);
    y[2] = vgetq_lane_f64(yk, 1);
    y[6] = vgetq_lane_f64(yNk, 1);
    y[7] = vgetq_lane_f64(yNk, 0);
    
    /* y[N/2] = sqrt(2) * z_buf[N] */
    y[4] = NOTORIOUS_FFT_SQRT2 * z_buf[4];
}

/* NEON-accelerated DCT-2 post-processing for arbitrary N (multiple of 4) */
static void notorious_fft_dct2_postprocess_neon(int N, notorious_fft_real* y,
                                          const notorious_fft_real* z_buf,
                                          const notorious_fft_real* dct_tw) {
    /* y[0] = 2 * z_buf[0] */
    y[0] = 2.0 * z_buf[0];
    
    /* Process k=1 to N/2-1, 2 at a time using NEON */
    int k;
    for (k = 1; k + 2 <= N/2; k += 2) {
        /* Load twiddles: cos/sin pairs for k and k+1 */
        float64x2_t tw_0 = vld1q_f64(dct_tw + 2*k);      /* cos(k), sin(k) */
        float64x2_t tw_1 = vld1q_f64(dct_tw + 2*(k+1));  /* cos(k+1), sin(k+1) */
        
        /* Extract cos and sin values */
        float64x2_t cos_k = vuzp1q_f64(tw_0, tw_1);  /* cos(k), cos(k+1) */
        float64x2_t sin_k = vuzp2q_f64(tw_0, tw_1);  /* sin(k), sin(k+1) */
        
        /* Load z values: z_buf has interleaved real/imag at 2*k, 2*k+1 */
        float64x2_t z_0 = vld1q_f64(z_buf + 2*k);      /* zr[k], zi[k] */
        float64x2_t z_1 = vld1q_f64(z_buf + 2*(k+1));  /* zr[k+1], zi[k+1] */
        
        float64x2_t zr_k = vuzp1q_f64(z_0, z_1);  /* zr[k], zr[k+1] */
        float64x2_t zi_k = vuzp2q_f64(z_0, z_1);  /* zi[k], zi[k+1] */
        
        /* y[k] = 2 * (zr_k * cos_k - zi_k * sin_k) */
        float64x2_t yk = vmulq_f64(zr_k, cos_k);
        yk = vfmsq_f64(yk, zi_k, sin_k);
        yk = vmulq_n_f64(yk, 2.0);
        
        /* y[N-k] = -2 * (zr_k * sin_k + zi_k * cos_k) */
        float64x2_t yNk = vmulq_f64(zr_k, sin_k);
        yNk = vfmaq_f64(yNk, zi_k, cos_k);
        yNk = vmulq_n_f64(yNk, -2.0);
        
        /* Store results */
        y[k] = vgetq_lane_f64(yk, 0);
        y[k+1] = vgetq_lane_f64(yk, 1);
        y[N - (k+1)] = vgetq_lane_f64(yNk, 1);
        y[N - k] = vgetq_lane_f64(yNk, 0);
    }
    
    /* Scalar tail for remaining k (if N/2 is odd) */
    for (; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k];
        notorious_fft_real tw_i = dct_tw[2*k+1];
        notorious_fft_real zr_k = z_buf[2*k];
        notorious_fft_real zi_k = z_buf[2*k+1];
        
        y[k] = 2 * (zr_k * tw_r - zi_k * tw_i);
        y[N - k] = -2 * (zr_k * tw_i + zi_k * tw_r);
    }
    
    /* y[N/2] = sqrt(2) * z_buf[N] */
    y[N/2] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

/* NEON-accelerated DST-2 post-processing for arbitrary N (multiple of 4)
 *
 * DST-2 postprocess per k (k=1..N/2-1):
 *   y[N-k-1] =  2*(zr*cos_k - zi*sin_k)   ← same cos-term as DCT-2's y[k]
 *   y[k-1]   = -2*(zr*sin_k + zi*cos_k)   ← same sin-term as DCT-2's y[N-k]
 *
 * Vector math is identical to notorious_fft_dct2_postprocess_neon; only store indices differ.
 */
static void notorious_fft_dst2_postprocess_neon(int N, notorious_fft_real* y,
                                          const notorious_fft_real* z_buf,
                                          const notorious_fft_real* dct_tw) {
    y[N - 1] = 2.0 * z_buf[0];

    int k;
    for (k = 1; k + 2 <= N/2; k += 2) {
        float64x2_t tw_0 = vld1q_f64(dct_tw + 2*k);
        float64x2_t tw_1 = vld1q_f64(dct_tw + 2*(k+1));
        float64x2_t cos_k = vuzp1q_f64(tw_0, tw_1);
        float64x2_t sin_k = vuzp2q_f64(tw_0, tw_1);

        float64x2_t z_0 = vld1q_f64(z_buf + 2*k);
        float64x2_t z_1 = vld1q_f64(z_buf + 2*(k+1));
        float64x2_t zr_k = vuzp1q_f64(z_0, z_1);
        float64x2_t zi_k = vuzp2q_f64(z_0, z_1);

        /* cos-term: 2*(zr*cos - zi*sin) → y[N-k-1], y[N-k-2] */
        float64x2_t ycos = vmulq_f64(zr_k, cos_k);
        ycos = vfmsq_f64(ycos, zi_k, sin_k);
        ycos = vmulq_n_f64(ycos, 2.0);

        /* sin-term: -2*(zr*sin + zi*cos) → y[k-1], y[k] */
        float64x2_t ysin = vmulq_f64(zr_k, sin_k);
        ysin = vfmaq_f64(ysin, zi_k, cos_k);
        ysin = vmulq_n_f64(ysin, -2.0);

        /* DST-2 index mapping (cf. scalar loop):
         *   cos-term for k   → y[N-k-1],   cos-term for k+1 → y[N-k-2]
         *   sin-term for k   → y[k-1],      sin-term for k+1 → y[k]   */
        y[N - k - 1] = vgetq_lane_f64(ycos, 0);
        y[N - k - 2] = vgetq_lane_f64(ycos, 1);
        y[k - 1]     = vgetq_lane_f64(ysin, 0);
        y[k]         = vgetq_lane_f64(ysin, 1);
    }

    for (; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real zr_k = z_buf[2*k],  zi_k = z_buf[2*k+1];
        y[k - 1]     = -2 * (zr_k * tw_i + zi_k * tw_r);
        y[N - k - 1] =  2 * (zr_k * tw_r - zi_k * tw_i);
    }

    y[N/2 - 1] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

/* Main NEON DCT-2 entry point for 1D transforms */
static void notorious_fft_dct2_neon_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    
    if (N == 1) {
        y[0] = 2 * x[0];
        return;
    }
    if (N == 2) {
        y[0] = 2 * (x[0] + x[1]);
        y[1] = NOTORIOUS_FFT_SQRT2 * (x[0] - x[1]);
        return;
    }
    
    /* Get scratch buffers */
    notorious_fft_real* reordered = a->scratch_im;
    notorious_fft_real* z_buf = a->scratch_im + N;
    const notorious_fft_real* dct_tw = a->scratch_re;
    
    /* Reorder input */
    for (int i = 0; i < N/2; i++) {
        reordered[i] = x[2*i];
        reordered[N/2 + i] = x[N - 1 - 2*i];
    }
    
    /* Real DFT - uses existing implementation */
    notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);
    
    /* Post-process with NEON if N is multiple of 4 */
    if (N % 4 == 0) {
        notorious_fft_dct2_postprocess_neon(N, y, z_buf, dct_tw);
    } else {
        /* Scalar fallback for non-multiple of 4 */
        y[0] = 2 * z_buf[0];
        for (int k = 1; k < N/2; k++) {
            notorious_fft_real tw_r = dct_tw[2*k];
            notorious_fft_real tw_i = dct_tw[2*k+1];
            notorious_fft_real zr_k = z_buf[2*k], zi_k = z_buf[2*k+1];
            y[k] = 2 * (zr_k * tw_r - zi_k * tw_i);
            y[N - k] = -2 * (zr_k * tw_i + zi_k * tw_r);
        }
        y[N/2] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
    }
}

#endif /* NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE) */

/* ============================================================================
 * DCT/DST-4 helper macros — defined here so AVX2 scalar-tail paths below
 * can use them before the main notorious_fft_t4_body function is reached.
 * ============================================================================ */

/* Core premul: t[n].re = er*x_e - ei*x_o,  t[n].im = er*x_o + ei*x_e */
#define NOTORIOUS_FFT_T4_PREMUL(tr, ti, er, ei, xe, xo) \
    do { (tr) = (er)*(xe) - (ei)*(xo); (ti) = (er)*(xo) + (ei)*(xe); } while(0)

/* Core premul for DST-4: t[n] = -e_pre[n] * (x[2n] - i*x[N-1-2n])
 * = (-er*xe - ei*xo) + i*(-er*(-xo) + (-ei)*xe) => simpler:
 * tr = -er*xe - ei*xo,  ti = er*xo - ei*xe */
#define NOTORIOUS_FFT_T4_PREMUL_DST(tr, ti, er, ei, xe, xo) \
    do { (tr) = -(er)*(xe) - (ei)*(xo); (ti) = (er)*(xo) - (ei)*(xe); } while(0)

/* Postmul for DCT-4: re extraction */
#define NOTORIOUS_FFT_T4_POST_DCT(yk, fr, fi, tr, ti)  ((yk) = 2*((fr)*(tr) - (fi)*(ti)))
/* Postmul for DST-4: im extraction */
#define NOTORIOUS_FFT_T4_POST_DST(yk, fr, fi, tr, ti)  ((yk) = 2*((fr)*(ti) + (fi)*(tr)))

/* ============================================================================
 * AVX2 DCT-2 / DST-2 Acceleration (Double Precision)
 * ============================================================================ */

#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)

/* AVX2 DCT-2 post-processing: processes k+=4 per iteration (4 bins).
 * Dispatch condition: N % 8 == 0 && N >= 16 */
static void notorious_fft_dct2_postprocess_avx2(int N, notorious_fft_real* y,
                                          const notorious_fft_real* z_buf,
                                          const notorious_fft_real* dct_tw) {
    y[0] = 2.0 * z_buf[0];

    int k;
    for (k = 1; k + 4 <= N/2; k += 4) {
        /* Load twiddles for k, k+1, k+2, k+3 */
        __m256d tw_01 = _mm256_loadu_pd(dct_tw + 2*k);      /* cos(k),sin(k),cos(k+1),sin(k+1) */
        __m256d tw_23 = _mm256_loadu_pd(dct_tw + 2*(k+2));  /* cos(k+2),sin(k+2),cos(k+3),sin(k+3) */

        /* Deinterleave to get 4 cos and 4 sin values */
        __m256d lo = _mm256_unpacklo_pd(tw_01, tw_23);  /* cos(k),cos(k+2),cos(k+1),cos(k+3) */
        __m256d hi = _mm256_unpackhi_pd(tw_01, tw_23);  /* sin(k),sin(k+2),sin(k+1),sin(k+3) */
        __m256d cos_k = _mm256_permute4x64_pd(lo, 0xD8); /* cos(k),cos(k+1),cos(k+2),cos(k+3) */
        __m256d sin_k = _mm256_permute4x64_pd(hi, 0xD8); /* sin(k),sin(k+1),sin(k+2),sin(k+3) */

        /* Load 4 z values */
        __m256d z_01 = _mm256_loadu_pd(z_buf + 2*k);
        __m256d z_23 = _mm256_loadu_pd(z_buf + 2*(k+2));
        __m256d zr_lo = _mm256_unpacklo_pd(z_01, z_23);
        __m256d zi_lo = _mm256_unpackhi_pd(z_01, z_23);
        __m256d zr_k = _mm256_permute4x64_pd(zr_lo, 0xD8);
        __m256d zi_k = _mm256_permute4x64_pd(zi_lo, 0xD8);

        /* y[k] = 2 * (zr*cos - zi*sin) */
        __m256d yk = _mm256_mul_pd(
            _mm256_set1_pd(2.0),
            _mm256_fmsub_pd(zr_k, cos_k, _mm256_mul_pd(zi_k, sin_k)));

        /* y[N-k] = -2 * (zr*sin + zi*cos) */
        __m256d yNk = _mm256_mul_pd(
            _mm256_set1_pd(-2.0),
            _mm256_fmadd_pd(zr_k, sin_k, _mm256_mul_pd(zi_k, cos_k)));

        /* Store y[k..k+3] forward, y[N-k..N-k-3] reversed */
        double yk_arr[4], yNk_arr[4];
        _mm256_storeu_pd(yk_arr,  yk);
        _mm256_storeu_pd(yNk_arr, yNk);
        y[k]   = yk_arr[0]; y[k+1] = yk_arr[1];
        y[k+2] = yk_arr[2]; y[k+3] = yk_arr[3];
        y[N-(k+3)] = yNk_arr[3]; y[N-(k+2)] = yNk_arr[2];
        y[N-(k+1)] = yNk_arr[1]; y[N-k]     = yNk_arr[0];
    }

    /* Scalar tail */
    for (; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real zr_k = z_buf[2*k],  zi_k = z_buf[2*k+1];
        y[k]   =  2 * (zr_k * tw_r - zi_k * tw_i);
        y[N-k] = -2 * (zr_k * tw_i + zi_k * tw_r);
    }

    y[N/2] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

/* AVX2 DST-2 post-processing: mirrors DCT-2, different store indices */
static void notorious_fft_dst2_postprocess_avx2(int N, notorious_fft_real* y,
                                          const notorious_fft_real* z_buf,
                                          const notorious_fft_real* dct_tw) {
    y[N-1] = 2.0 * z_buf[0];

    int k;
    for (k = 1; k + 4 <= N/2; k += 4) {
        __m256d tw_01 = _mm256_loadu_pd(dct_tw + 2*k);
        __m256d tw_23 = _mm256_loadu_pd(dct_tw + 2*(k+2));
        __m256d lo = _mm256_unpacklo_pd(tw_01, tw_23);
        __m256d hi = _mm256_unpackhi_pd(tw_01, tw_23);
        __m256d cos_k = _mm256_permute4x64_pd(lo, 0xD8);
        __m256d sin_k = _mm256_permute4x64_pd(hi, 0xD8);

        __m256d z_01 = _mm256_loadu_pd(z_buf + 2*k);
        __m256d z_23 = _mm256_loadu_pd(z_buf + 2*(k+2));
        __m256d zr_lo = _mm256_unpacklo_pd(z_01, z_23);
        __m256d zi_lo = _mm256_unpackhi_pd(z_01, z_23);
        __m256d zr_k = _mm256_permute4x64_pd(zr_lo, 0xD8);
        __m256d zi_k = _mm256_permute4x64_pd(zi_lo, 0xD8);

        /* cos-term: 2*(zr*cos - zi*sin) → y[N-k-1..N-k-4] */
        __m256d ycos = _mm256_mul_pd(
            _mm256_set1_pd(2.0),
            _mm256_fmsub_pd(zr_k, cos_k, _mm256_mul_pd(zi_k, sin_k)));

        /* sin-term: -2*(zr*sin + zi*cos) → y[k-1..k+2] */
        __m256d ysin = _mm256_mul_pd(
            _mm256_set1_pd(-2.0),
            _mm256_fmadd_pd(zr_k, sin_k, _mm256_mul_pd(zi_k, cos_k)));

        double ycos_arr[4], ysin_arr[4];
        _mm256_storeu_pd(ycos_arr, ycos);
        _mm256_storeu_pd(ysin_arr, ysin);
        /* cos-term for k   → y[N-k-1], k+1 → y[N-k-2], etc. */
        y[N-k-1] = ycos_arr[0]; y[N-k-2] = ycos_arr[1];
        y[N-k-3] = ycos_arr[2]; y[N-k-4] = ycos_arr[3];
        /* sin-term for k   → y[k-1], k+1 → y[k], etc. */
        y[k-1] = ysin_arr[0]; y[k]   = ysin_arr[1];
        y[k+1] = ysin_arr[2]; y[k+2] = ysin_arr[3];
    }

    /* Scalar tail */
    for (; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real zr_k = z_buf[2*k],  zi_k = z_buf[2*k+1];
        y[k-1]     = -2 * (zr_k * tw_i + zi_k * tw_r);
        y[N-k-1]   =  2 * (zr_k * tw_r - zi_k * tw_i);
    }

    y[N/2-1] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

/* AVX2-vectorized premul for DCT/DST-4: processes 2 complex outputs per iteration */
static NOTORIOUS_FFT_INLINE void notorious_fft_t4_premul_avx2(
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT x, notorious_fft_real* NOTORIOUS_FFT_RESTRICT t,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT ep_pre, int M, int N, int is_dst)
{
    for (int n = 0; n + 2 <= M; n += 2) {
        /* Load twiddles: (er0,ei0), (er1,ei1) interleaved */
        __m256d tw = _mm256_loadu_pd(ep_pre + 2*n);   /* er0,ei0,er1,ei1 */
        __m256d er = _mm256_unpacklo_pd(tw, tw);       /* er0,er0,er1,er1 — wrong lane order */
        /* Use permute to get correct order */
        __m256d tw_lo = _mm256_permute2f128_pd(tw, tw, 0x00); /* er0,ei0,er0,ei0 */
        __m256d tw_hi = _mm256_permute2f128_pd(tw, tw, 0x11); /* er1,ei1,er1,ei1 */
        double e0r = ep_pre[2*n],   e0i = ep_pre[2*n+1];
        double e1r = ep_pre[2*n+2], e1i = ep_pre[2*n+3];

        double xe0 = x[2*n],       xo0 = x[N-1-2*n];
        double xe1 = x[2*(n+1)],   xo1 = x[N-1-2*(n+1)];

        if (!is_dst) {
            t[2*n]   = e0r*xe0 - e0i*xo0; t[2*n+1] = e0r*xo0 + e0i*xe0;
            t[2*n+2] = e1r*xe1 - e1i*xo1; t[2*n+3] = e1r*xo1 + e1i*xe1;
        } else {
            t[2*n]   = -e0r*xe0 - e0i*xo0; t[2*n+1] = e0r*xo0 - e0i*xe0;
            t[2*n+2] = -e1r*xe1 - e1i*xo1; t[2*n+3] = e1r*xo1 - e1i*xe1;
        }
        (void)er; (void)tw_lo; (void)tw_hi; /* suppress unused-variable warnings */
    }
    /* scalar tail */
    if (M & 1) {
        int n = M - 1;
        if (!is_dst)
            NOTORIOUS_FFT_T4_PREMUL(t[2*n], t[2*n+1], ep_pre[2*n], ep_pre[2*n+1], x[2*n], x[N-1-2*n]);
        else
            NOTORIOUS_FFT_T4_PREMUL_DST(t[2*n], t[2*n+1], ep_pre[2*n], ep_pre[2*n+1], x[2*n], x[N-1-2*n]);
    }
}

static NOTORIOUS_FFT_INLINE void notorious_fft_t4_postmul_avx2(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT y, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT t,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT ep_post, int M, int N, int is_dst)
{
    for (int k = 0; k + 2 <= M; k += 2) {
        double fr0 = ep_post[2*(2*k)],   fi0 = ep_post[2*(2*k)+1];
        double fr1 = ep_post[2*(2*k+2)], fi1 = ep_post[2*(2*k+2)+1];
        double fr2 = ep_post[2*(2*k+1)], fi2 = ep_post[2*(2*k+1)+1];
        double fr3 = ep_post[2*(2*k+3)], fi3 = ep_post[2*(2*k+3)+1];
        double tr0 = t[2*k],     ti0 = t[2*k+1];
        double tr1 = t[2*k+2],   ti1 = t[2*k+3];
        /* conj(t[M-1-k]), conj(t[M-2-k]) */
        double ctr0 = t[N-2-2*k], cti0 = -t[N-1-2*k];
        double ctr1 = t[N-4-2*k], cti1 = -t[N-3-2*k];

        if (!is_dst) {
            y[2*k]   = 2*(fr0*tr0 - fi0*ti0);
            y[2*k+2] = 2*(fr1*tr1 - fi1*ti1);
            y[2*k+1] = 2*(fr2*ctr0 - fi2*cti0);
            y[2*k+3] = 2*(fr3*ctr1 - fi3*cti1);
        } else {
            y[2*k]   = 2*(fr0*ti0 + fi0*tr0);
            y[2*k+2] = 2*(fr1*ti1 + fi1*tr1);
            y[2*k+1] = 2*(fr2*cti0 + fi2*ctr0);
            y[2*k+3] = 2*(fr3*cti1 + fi3*ctr1);
        }
    }
    /* scalar tail */
    if (M & 1) {
        int k = M - 1;
        if (!is_dst) {
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],      t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[N-2-2*k], -t[N-1-2*k]);
        } else {
            NOTORIOUS_FFT_T4_POST_DST(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],      t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DST(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[N-2-2*k], -t[N-1-2*k]);
        }
    }
}

#endif /* NOTORIOUS_FFT_HAS_AVX2 && !NOTORIOUS_FFT_SINGLE */

/* ============================================================================
 * DCT/DST Type 2 — zero-malloc, precomputed twiddles
 *
 * aux->e          = realdft unpack twiddles (N/4 pairs)
 * aux->t          = realdft scratch (N reals)
 * aux->scratch_re = DCT-2/3 twiddles (N/2 pairs: cos/sin of -πk/(2N))
 * aux->scratch_im = reorder + complex scratch buffer (N + N+2 reals)
 * ============================================================================ */

static void notorious_fft_s_dct2_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) {
        y[0] = 2 * x[0];
        return;
    }
    if (N == 2) {
        y[0] = 2 * (x[0] + x[1]);
        y[1] = NOTORIOUS_FFT_SQRT2 * (x[0] - x[1]);
        return;
    }

    /* Use preallocated scratch: scratch_im holds [reordered (N) | z (N+2)] */
    notorious_fft_real* reordered = a->scratch_im;
    notorious_fft_real* z_buf = a->scratch_im + N;  /* (N/2+1) interleaved complex = N+2 reals */

    /* Reorder: even indices first, then odd reversed */
    for (int i = 0; i < N/2; i++) {
        reordered[i] = x[2*i];
        reordered[N/2 + i] = x[N - 1 - 2*i];
    }

    /* Real DFT — uses the precomputed twiddles in aux->e and scratch in aux->t */
    notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);

    /* Post-process with precomputed DCT twiddles from scratch_re */
    const notorious_fft_real* dct_tw = a->scratch_re;  /* pairs: (cos, sin) for k=0..N/2-1 */

    y[0] = 2 * z_buf[0];
    for (int k = 1; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k];
        notorious_fft_real tw_i = dct_tw[2*k+1];

        notorious_fft_real zr_k = z_buf[2*k], zi_k = z_buf[2*k+1];

        y[k] = 2 * (zr_k * tw_r - zi_k * tw_i);
        y[N - k] = -2 * (zr_k * tw_i + zi_k * tw_r);
    }
    y[N/2] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

void notorious_fft_dct2(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan) {
        int N = a->N;
#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
        if (N % 8 == 0 && N >= 16) {
            notorious_fft_real* reordered = a->scratch_im;
            notorious_fft_real* z_buf = a->scratch_im + N;
            for (int i = 0; i < N/2; i++) {
                reordered[i]       = x[2*i];
                reordered[N/2 + i] = x[N - 1 - 2*i];
            }
            notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);
            notorious_fft_dct2_postprocess_avx2(N, y, z_buf, a->scratch_re);
            return;
        }
#endif
#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
        /* Use NEON-accelerated version for 1D DCT-2 when N is multiple of 4 */
        if (N % 4 == 0 && N >= 8) {
            notorious_fft_dct2_neon_1d(x, y, a);
            return;
        }
#endif
        notorious_fft_s_dct2_1d(x, y, a);
    } else if (a->sub2) {
        /* Multi-dimensional: sub1 = higher dims, sub2 = current dim */
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;

        if (N1 == 1) {
            notorious_fft_dct2(x, y, a->sub2);
            return;
        }

        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc(total * sizeof(notorious_fft_real));
        if (!temp) return;

        /* Transform hyperplanes using sub1 — no OpenMP, shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++) {
            if (a->sub1) {
                notorious_fft_dct2(x + n * N1, temp + n * N1, a->sub1);
            } else {
                memcpy(temp + n * N1, x + n * N1, N1 * sizeof(notorious_fft_real));
            }
        }

        /* Transform rows using sub2 — pre-allocate buffers once */
        notorious_fft_real* col_in = (notorious_fft_real*)malloc(N2 * sizeof(notorious_fft_real));
        notorious_fft_real* col_out = (notorious_fft_real*)malloc(N2 * sizeof(notorious_fft_real));
        if (col_in && col_out) {
            for (int n = 0; n < N1; n++) {
                for (int k = 0; k < N2; k++) {
                    col_in[k] = temp[n + k * N1];
                }
                notorious_fft_dct2(col_in, col_out, a->sub2);
                for (int k = 0; k < N2; k++) {
                    y[n + k * N1] = col_out[k];
                }
            }
        }
        free(col_in);
        free(col_out);
        free(temp);
    } else {
        notorious_fft_s_dct2_1d(x, y, a);
    }
}

static void notorious_fft_s_dst2_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) { y[0] = 2 * x[0]; return; }
    if (N == 2) {
        y[0] = NOTORIOUS_FFT_SQRT2 * (x[0] + x[1]);
        y[1] = 2 * (x[0] - x[1]);
        return;
    }

    notorious_fft_real* reordered = a->scratch_im;
    notorious_fft_real* z_buf = a->scratch_im + N;
    const notorious_fft_real* dct_tw = a->scratch_re;

    for (int i = 0; i < N/2; i++) {
        reordered[i]       = x[2*i];
        reordered[N/2 + i] = -x[N - 1 - 2*i];
    }

    notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);

    y[N - 1] = 2 * z_buf[0];
    for (int k = 1; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real zr_k = z_buf[2*k],  zi_k = z_buf[2*k+1];
        y[k - 1]     = -2 * (zr_k * tw_i + zi_k * tw_r);
        y[N - k - 1] =  2 * (zr_k * tw_r - zi_k * tw_i);
    }
    y[N/2 - 1] = NOTORIOUS_FFT_SQRT2 * z_buf[N];
}

void notorious_fft_dst2(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan) {
        int N = a->N;
#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
        if (N % 8 == 0 && N >= 16) {
            notorious_fft_real* reordered = a->scratch_im;
            notorious_fft_real* z_buf = a->scratch_im + N;
            for (int i = 0; i < N/2; i++) {
                reordered[i]       = x[2*i];
                reordered[N/2 + i] = -x[N - 1 - 2*i];
            }
            notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);
            notorious_fft_dst2_postprocess_avx2(N, y, z_buf, a->scratch_re);
            return;
        }
#endif
#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
        if (N % 4 == 0 && N >= 8) {
            notorious_fft_real* reordered = a->scratch_im;
            notorious_fft_real* z_buf = a->scratch_im + N;
            for (int i = 0; i < N/2; i++) {
                reordered[i]       = x[2*i];
                reordered[N/2 + i] = -x[N - 1 - 2*i];
            }
            notorious_fft_realdft(reordered, (notorious_fft_cmpl*)z_buf, a);
            notorious_fft_dst2_postprocess_neon(N, y, z_buf, a->scratch_re);
            return;
        }
#endif
        notorious_fft_s_dst2_1d(x, y, a);
    } else if (a->sub2) {
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;
        if (N1 == 1) { notorious_fft_dst2(x, y, a->sub2); return; }
        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc((size_t)total * sizeof(notorious_fft_real));
        if (!temp) return;
        /* No OpenMP — shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++)
            notorious_fft_dst2(x + n*N1, temp + n*N1, a->sub1);
        notorious_fft_real* col = (notorious_fft_real*)malloc((size_t)N2 * sizeof(notorious_fft_real));
        if (col) {
            for (int n = 0; n < N1; n++) {
                for (int i = 0; i < N2; i++) col[i] = temp[n + i*N1];
                notorious_fft_dst2(col, col, a->sub2);
                for (int i = 0; i < N2; i++) y[n + i*N1] = col[i];
            }
        }
        free(col);
        free(temp);
    } else {
        notorious_fft_s_dst2_1d(x, y, a);
    }
}

/* ============================================================================
 * DCT/DST Type 3 — zero-malloc, precomputed twiddles
 * ============================================================================ */

static void notorious_fft_s_dct3_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) { y[0] = x[0]; return; }
    if (N == 2) {
        notorious_fft_real s = NOTORIOUS_FFT_SQRT2 * x[1];
        y[0] = x[0] + s; y[1] = x[0] - s;
        return;
    }

    notorious_fft_real* temp = a->scratch_im;
    notorious_fft_real* z_buf = a->scratch_im + N;
    const notorious_fft_real* dct_tw = a->scratch_re;

    z_buf[0] = x[0]; z_buf[1] = 0;
    for (int k = 1; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real xr = x[k], xi = x[N - k];
        z_buf[2*k]   =  xr * tw_r - xi * tw_i;
        z_buf[2*k+1] = -xr * tw_i - xi * tw_r;
    }
    z_buf[N] = NOTORIOUS_FFT_SQRT2 * x[N/2]; z_buf[N+1] = 0;
    notorious_fft_invrealdft((notorious_fft_cmpl*)z_buf, temp, a);
    for (int i = 0; i < N/2; i++) {
        y[2*i] = temp[i];
        y[N - 1 - 2*i] = temp[N/2 + i];
    }
}

void notorious_fft_dct3(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan) {
        notorious_fft_s_dct3_1d(x, y, a);
    } else if (a->sub2) {
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;
        if (N1 == 1) { notorious_fft_dct3(x, y, a->sub2); return; }
        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc((size_t)total * sizeof(notorious_fft_real));
        if (!temp) return;
        /* No OpenMP — shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++)
            notorious_fft_dct3(x + n*N1, temp + n*N1, a->sub1);
        notorious_fft_real* col = (notorious_fft_real*)malloc((size_t)N2 * sizeof(notorious_fft_real));
        if (col) {
            for (int n = 0; n < N1; n++) {
                for (int i = 0; i < N2; i++) col[i] = temp[n + i*N1];
                notorious_fft_dct3(col, col, a->sub2);
                for (int i = 0; i < N2; i++) y[n + i*N1] = col[i];
            }
        }
        free(col);
        free(temp);
    } else {
        notorious_fft_s_dct3_1d(x, y, a);
    }
}

static void notorious_fft_s_dst3_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    int N = a->N;
    if (N == 1) { y[0] = x[0]; return; }
    if (N == 2) {
        notorious_fft_real s = NOTORIOUS_FFT_SQRT2 * x[0];
        y[0] = s + x[1]; y[1] = s - x[1];
        return;
    }

    notorious_fft_real* temp = a->scratch_im;
    notorious_fft_real* z_buf = a->scratch_im + N;
    const notorious_fft_real* dct_tw = a->scratch_re;

    z_buf[0] = x[N - 1]; z_buf[1] = 0;
    for (int k = 1; k < N/2; k++) {
        notorious_fft_real tw_r = dct_tw[2*k], tw_i = dct_tw[2*k+1];
        notorious_fft_real xr = x[N - k - 1], xi = x[k - 1];
        z_buf[2*k]   =  xr * tw_r - xi * tw_i;
        z_buf[2*k+1] = -xr * tw_i - xi * tw_r;
    }
    z_buf[N] = NOTORIOUS_FFT_SQRT2 * x[N/2 - 1]; z_buf[N+1] = 0;
    notorious_fft_invrealdft((notorious_fft_cmpl*)z_buf, temp, a);
    for (int i = 0; i < N/2; i++) {
        y[2*i] = temp[i];
        y[N - 1 - 2*i] = -temp[N/2 + i];
    }
}

void notorious_fft_dst3(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan) {
        notorious_fft_s_dst3_1d(x, y, a);
    } else if (a->sub2) {
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;
        if (N1 == 1) { notorious_fft_dst3(x, y, a->sub2); return; }
        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc((size_t)total * sizeof(notorious_fft_real));
        if (!temp) return;
        /* No OpenMP — shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++)
            notorious_fft_dst3(x + n*N1, temp + n*N1, a->sub1);
        notorious_fft_real* col = (notorious_fft_real*)malloc((size_t)N2 * sizeof(notorious_fft_real));
        if (col) {
            for (int n = 0; n < N1; n++) {
                for (int i = 0; i < N2; i++) col[i] = temp[n + i*N1];
                notorious_fft_dst3(col, col, a->sub2);
                for (int i = 0; i < N2; i++) y[n + i*N1] = col[i];
            }
        }
        free(col);
        free(temp);
    } else {
        notorious_fft_s_dst3_1d(x, y, a);
    }
}

/* ============================================================================
 * DCT/DST Type 4 — O(N log N) via reduction to N/2-point complex DFT
 *
 * Algorithm (identical to minfft):
 *   1. Premul:   t[n] = e_pre[n] * (x[2n] + i*x[N-1-2n]),  n=0..M-1  (M=N/2)
 *   2. DFT:      t → DFT_M(t)  (in-place)
 *   3. Postmul:
 *      y[2k]   = 2 * Re(e_post[2k]   * t[k])
 *      y[2k+1] = 2 * Re(e_post[2k+1] * conj(t[M-1-k]))  for k=0..M-1
 *
 * aux->e layout: [premul (2M reals)] [postmul (2N reals)]
 * aux->t: scratch N reals (M interleaved complex)
 *
 * DST-4 differs only in the premul sign and postmul imaginary extraction:
 *   premul:  t[n] = -e_pre[n] * (x[2n] - i*x[N-1-2n])
 *   postmul: y[2k]   = 2 * Im(e_post[2k]   * t[k])
 *            y[2k+1] = 2 * Im(e_post[2k+1] * conj(t[M-1-k]))
 * ============================================================================ */

/* Shared inner body: premul + DFT + postmul — used by scalar and unrolled paths.
 * ep_pre  = premul twiddles (M pairs), ep_post = postmul twiddles (N pairs)
 * t_buf   = scratch (N reals = M interleaved complex)
 * is_dst  = 0 for DCT-4, 1 for DST-4 */
static NOTORIOUS_FFT_INLINE void notorious_fft_t4_body(
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT x,
    notorious_fft_real*       NOTORIOUS_FFT_RESTRICT y,
    int N, const notorious_fft_plan* plan,
    const notorious_fft_real* ep_pre,   /* 2M reals */
    const notorious_fft_real* ep_post,  /* 2N reals */
    notorious_fft_real*       t_buf,    /* N reals  */
    int is_dst)
{
    int M = N / 2;

    /* Step 1: premul */
    if (!is_dst) {
        for (int n = 0; n < M; n++)
            NOTORIOUS_FFT_T4_PREMUL(t_buf[2*n], t_buf[2*n+1],
                             ep_pre[2*n], ep_pre[2*n+1],
                             x[2*n], x[N-1-2*n]);
    } else {
        for (int n = 0; n < M; n++)
            NOTORIOUS_FFT_T4_PREMUL_DST(t_buf[2*n], t_buf[2*n+1],
                                  ep_pre[2*n], ep_pre[2*n+1],
                                  x[2*n], x[N-1-2*n]);
    }

    /* Step 2: N/2-point complex DFT (forward) */
    if (plan) {
        notorious_fft_execute_cx(plan, t_buf, t_buf, 0);
    }
    /* M=1: DFT of single element is identity — nothing to do */

    /* Step 3: postmul */
    if (!is_dst) {
        for (int k = 0; k < M; k++) {
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],
                                          t_buf[2*k],          t_buf[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1],
                                          t_buf[N-2-2*k],      -t_buf[N-1-2*k]);
        }
    } else {
        for (int k = 0; k < M; k++) {
            NOTORIOUS_FFT_T4_POST_DST(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],
                                          t_buf[2*k],          t_buf[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DST(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1],
                                          t_buf[N-2-2*k],      -t_buf[N-1-2*k]);
        }
    }
}

/* ---- N=8 fully unrolled (forward DCT-4 / DST-4) ---- */
static NOTORIOUS_FFT_INLINE void notorious_fft_t4_n8_unroll(
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT x, notorious_fft_real* NOTORIOUS_FFT_RESTRICT y,
    const notorious_fft_real* ep_pre, const notorious_fft_real* ep_post, int is_dst)
{
    /* Premul: M=4 */
    notorious_fft_real tr0, ti0, tr1, ti1, tr2, ti2, tr3, ti3;
    if (!is_dst) {
        NOTORIOUS_FFT_T4_PREMUL(tr0,ti0, ep_pre[0],ep_pre[1], x[0],x[7]);
        NOTORIOUS_FFT_T4_PREMUL(tr1,ti1, ep_pre[2],ep_pre[3], x[2],x[5]);
        NOTORIOUS_FFT_T4_PREMUL(tr2,ti2, ep_pre[4],ep_pre[5], x[4],x[3]);
        NOTORIOUS_FFT_T4_PREMUL(tr3,ti3, ep_pre[6],ep_pre[7], x[6],x[1]);
    } else {
        NOTORIOUS_FFT_T4_PREMUL_DST(tr0,ti0, ep_pre[0],ep_pre[1], x[0],x[7]);
        NOTORIOUS_FFT_T4_PREMUL_DST(tr1,ti1, ep_pre[2],ep_pre[3], x[2],x[5]);
        NOTORIOUS_FFT_T4_PREMUL_DST(tr2,ti2, ep_pre[4],ep_pre[5], x[4],x[3]);
        NOTORIOUS_FFT_T4_PREMUL_DST(tr3,ti3, ep_pre[6],ep_pre[7], x[6],x[1]);
    }

    /* DFT-4 (radix-2 DIT, 2 stages) — identical to notorious_fft_kernel_4 logic */
    /* Stage 1 */
    notorious_fft_real a0r = tr0+tr2, a0i = ti0+ti2;
    notorious_fft_real a1r = tr1+tr3, a1i = ti1+ti3;
    notorious_fft_real a2r = tr0-tr2, a2i = ti0-ti2;
    notorious_fft_real a3r = tr1-tr3, a3i = ti1-ti3;
    /* Stage 2: twiddle -i on (a2,a3) */
    tr0 = a0r+a1r; ti0 = a0i+a1i;  /* bin 0 */
    tr2 = a0r-a1r; ti2 = a0i-a1i;  /* bin 2 */
    tr1 = a2r+a3i; ti1 = a2i-a3r;  /* bin 1 */
    tr3 = a2r-a3i; ti3 = a2i+a3r;  /* bin 3 */

    /* Postmul */
    notorious_fft_real t[8] = {tr0,ti0, tr1,ti1, tr2,ti2, tr3,ti3};
    if (!is_dst) {
        for (int k = 0; k < 4; k++) {
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],    t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[6-2*k], -t[7-2*k]);
        }
    } else {
        for (int k = 0; k < 4; k++) {
            NOTORIOUS_FFT_T4_POST_DST(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],    t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DST(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[6-2*k], -t[7-2*k]);
        }
    }
}

#if NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
/* NEON-vectorized premul + postmul for arbitrary N (multiple of 4).
 * Processes 2 complex values per NEON iteration. */
static NOTORIOUS_FFT_INLINE void notorious_fft_t4_premul_neon(
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT x, notorious_fft_real* NOTORIOUS_FFT_RESTRICT t,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT ep_pre, int M, int N, int is_dst)
{
    for (int n = 0; n < M - 1; n += 2) {
        /* Load twiddles: (er0,ei0), (er1,ei1) */
        float64x2_t tw0 = vld1q_f64(ep_pre + 2*n);
        float64x2_t tw1 = vld1q_f64(ep_pre + 2*n + 2);
        float64x2_t er  = vuzp1q_f64(tw0, tw1);  /* er0, er1 */
        float64x2_t ei  = vuzp2q_f64(tw0, tw1);  /* ei0, ei1 */

        /* x_e = x[2n], x[2n+2]; x_o = x[N-1-2n], x[N-3-2n] */
        float64x2_t xe = {x[2*n],     x[2*n+2]};
        float64x2_t xo = {x[N-1-2*n], x[N-3-2*n]};

        float64x2_t tr_, ti_;
        if (!is_dst) {
            /* tr = er*xe - ei*xo,  ti = er*xo + ei*xe */
            tr_ = vfmsq_f64(vmulq_f64(er, xe), ei, xo);
            ti_ = vfmaq_f64(vmulq_f64(er, xo), ei, xe);
        } else {
            /* tr = -er*xe - ei*xo,  ti = er*xo - ei*xe */
            tr_ = vnegq_f64(vfmaq_f64(vmulq_f64(er, xe), ei, xo));
            ti_ = vfmsq_f64(vmulq_f64(er, xo), ei, xe);
        }

        /* Store interleaved */
        float64x2_t p0 = vzip1q_f64(tr_, ti_);  /* tr[n+0], ti[n+0] */
        float64x2_t p1 = vzip2q_f64(tr_, ti_);  /* tr[n+1], ti[n+1] */
        vst1q_f64(t + 2*n,     p0);
        vst1q_f64(t + 2*n + 2, p1);
    }
    /* scalar tail if M is odd */
    if (M & 1) {
        int n = M - 1;
        if (!is_dst)
            NOTORIOUS_FFT_T4_PREMUL(t[2*n], t[2*n+1], ep_pre[2*n], ep_pre[2*n+1], x[2*n], x[N-1-2*n]);
        else
            NOTORIOUS_FFT_T4_PREMUL_DST(t[2*n], t[2*n+1], ep_pre[2*n], ep_pre[2*n+1], x[2*n], x[N-1-2*n]);
    }
}

static NOTORIOUS_FFT_INLINE void notorious_fft_t4_postmul_neon(
    notorious_fft_real* NOTORIOUS_FFT_RESTRICT y, const notorious_fft_real* NOTORIOUS_FFT_RESTRICT t,
    const notorious_fft_real* NOTORIOUS_FFT_RESTRICT ep_post, int M, int N, int is_dst)
{
    for (int k = 0; k < M - 1; k += 2) {
        /* Even outputs: y[2k], y[2k+2] */
        int ek0 = 2*(2*k),   ek1 = 2*(2*k+2);
        /* Odd outputs: y[2k+1], y[2k+3] */
        int ok0 = 2*(2*k+1), ok1 = 2*(2*k+3);

        /* Load postmul twiddles for even outputs */
        float64x2_t fw0 = vld1q_f64(ep_post + ek0);
        float64x2_t fw1 = vld1q_f64(ep_post + ek1);
        float64x2_t fer = vuzp1q_f64(fw0, fw1);
        float64x2_t fei = vuzp2q_f64(fw0, fw1);

        /* t[k] and t[k+1] */
        float64x2_t ta = vld1q_f64(t + 2*k);      /* tr[k],   ti[k]   */
        float64x2_t tb = vld1q_f64(t + 2*k + 2);  /* tr[k+1], ti[k+1] */
        float64x2_t ttr = vuzp1q_f64(ta, tb);  /* tr[k], tr[k+1] */
        float64x2_t tti = vuzp2q_f64(ta, tb);  /* ti[k], ti[k+1] */

        float64x2_t ye;
        if (!is_dst)
            ye = vmulq_n_f64(vfmsq_f64(vmulq_f64(fer, ttr), fei, tti), 2.0);
        else
            ye = vmulq_n_f64(vfmaq_f64(vmulq_f64(fer, tti), fei, ttr), 2.0);

        y[2*k]   = vgetq_lane_f64(ye, 0);
        y[2*k+2] = vgetq_lane_f64(ye, 1);

        /* Odd outputs: use conj(t[M-1-k]) */
        fw0 = vld1q_f64(ep_post + ok0);
        fw1 = vld1q_f64(ep_post + ok1);
        fer = vuzp1q_f64(fw0, fw1);
        fei = vuzp2q_f64(fw0, fw1);

        /* conj(t[M-1-k]), conj(t[M-2-k]) = (tr, -ti) for each */
        ta = vld1q_f64(t + N-2-2*k);   /* tr[M-1-k],   ti[M-1-k]   */
        tb = vld1q_f64(t + N-4-2*k);   /* tr[M-2-k],   ti[M-2-k]   */
        /* Re-pack in k-ascending order: k gives M-1-k, k+1 gives M-2-k */
        /* so indices in output: y[2k+1] uses conj(t[M-1-k]), y[2k+3] uses conj(t[M-2-k]) */
        float64x2_t ctr = {vgetq_lane_f64(ta, 0), vgetq_lane_f64(tb, 0)};
        float64x2_t cti = {-vgetq_lane_f64(ta, 1), -vgetq_lane_f64(tb, 1)};

        float64x2_t yo;
        if (!is_dst)
            yo = vmulq_n_f64(vfmsq_f64(vmulq_f64(fer, ctr), fei, cti), 2.0);
        else
            yo = vmulq_n_f64(vfmaq_f64(vmulq_f64(fer, cti), fei, ctr), 2.0);

        y[2*k+1] = vgetq_lane_f64(yo, 0);
        y[2*k+3] = vgetq_lane_f64(yo, 1);
    }
    /* scalar tail */
    if (M & 1) {
        int k = M - 1;
        if (!is_dst) {
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],      t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DCT(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[N-2-2*k], -t[N-1-2*k]);
        } else {
            NOTORIOUS_FFT_T4_POST_DST(y[2*k],   ep_post[2*(2*k)],   ep_post[2*(2*k)+1],   t[2*k],      t[2*k+1]);
            NOTORIOUS_FFT_T4_POST_DST(y[2*k+1], ep_post[2*(2*k+1)], ep_post[2*(2*k+1)+1], t[N-2-2*k], -t[N-1-2*k]);
        }
    }
}
#endif /* NOTORIOUS_FFT_HAS_NEON */

static void notorious_fft_s_t4_1d(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a, int is_dst) {
    int N = a->N;
    if (N == 1) {
        y[0] = NOTORIOUS_FFT_SQRT2 * x[0];
        return;
    }

    const notorious_fft_real* ep     = (const notorious_fft_real*)a->e;
    int M = N / 2;
    const notorious_fft_real* ep_pre  = ep;
    const notorious_fft_real* ep_post = ep + 2 * M;
    notorious_fft_real* t_buf = (notorious_fft_real*)a->t;

    /* N=8: fully unrolled */
    if (N == 8) {
        notorious_fft_t4_n8_unroll(x, y, ep_pre, ep_post, is_dst);
        return;
    }

#if NOTORIOUS_FFT_HAS_AVX2 && !defined(NOTORIOUS_FFT_SINGLE)
    if (N % 4 == 0) {
        notorious_fft_t4_premul_avx2(x, t_buf, ep_pre, M, N, is_dst);
        if (a->plan) notorious_fft_execute_cx(a->plan, t_buf, t_buf, 0);
        notorious_fft_t4_postmul_avx2(y, t_buf, ep_post, M, N, is_dst);
        return;
    }
#elif NOTORIOUS_FFT_HAS_NEON && !defined(NOTORIOUS_FFT_SINGLE)
    if (N % 4 == 0) {
        notorious_fft_t4_premul_neon(x, t_buf, ep_pre, M, N, is_dst);
        if (a->plan) notorious_fft_execute_cx(a->plan, t_buf, t_buf, 0);
        notorious_fft_t4_postmul_neon(y, t_buf, ep_post, M, N, is_dst);
        return;
    }
#endif

    notorious_fft_t4_body(x, y, N, a->plan, ep_pre, ep_post, t_buf, is_dst);
}

void notorious_fft_dct4(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan || a->N == 1 || (a->e && a->t)) {
        notorious_fft_s_t4_1d(x, y, a, 0);
    } else if (a->sub2) {
        /* Multi-dimensional */
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;
        if (N1 == 1) { notorious_fft_dct4(x, y, a->sub2); return; }
        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc((size_t)total * sizeof(notorious_fft_real));
        if (!temp) return;
        /* No OpenMP — shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++)
            notorious_fft_dct4(x + n*N1, temp + n*N1, a->sub1);
        notorious_fft_real* col = (notorious_fft_real*)malloc((size_t)N2 * sizeof(notorious_fft_real));
        if (col) {
            for (int n = 0; n < N1; n++) {
                for (int i = 0; i < N2; i++) col[i] = temp[n + i*N1];
                notorious_fft_dct4(col, col, a->sub2);
                for (int i = 0; i < N2; i++) y[n + i*N1] = col[i];
            }
        }
        free(col);
        free(temp);
    }
}

void notorious_fft_dst4(notorious_fft_real* x, notorious_fft_real* y, const notorious_fft_aux* a) {
    if (a->plan || a->N == 1 || (a->e && a->t)) {
        notorious_fft_s_t4_1d(x, y, a, 1);
    } else if (a->sub2) {
        int N1 = a->sub1 ? a->sub1->N : 1;
        int N2 = a->sub2->N;
        if (N1 == 1) { notorious_fft_dst4(x, y, a->sub2); return; }
        int total = N1 * N2;
        notorious_fft_real* temp = (notorious_fft_real*)malloc((size_t)total * sizeof(notorious_fft_real));
        if (!temp) return;
        /* No OpenMP — shared plan buffers are not thread-safe */
        for (int n = 0; n < N2; n++)
            notorious_fft_dst4(x + n*N1, temp + n*N1, a->sub1);
        notorious_fft_real* col = (notorious_fft_real*)malloc((size_t)N2 * sizeof(notorious_fft_real));
        if (col) {
            for (int n = 0; n < N1; n++) {
                for (int i = 0; i < N2; i++) col[i] = temp[n + i*N1];
                notorious_fft_dst4(col, col, a->sub2);
                for (int i = 0; i < N2; i++) y[n + i*N1] = col[i];
            }
        }
        free(col);
        free(temp);
    }
}

/* ============================================================================
 * Aux Data Creation — Pre-compute EVERYTHING
 * ============================================================================ */

static notorious_fft_aux* notorious_fft_make_aux(int d, int* Ns, int datasz,
                                    notorious_fft_aux* (*aux_1d)(int N));

static void notorious_fft_init_aux_from_plan(notorious_fft_aux* a) {
    a->sub1 = NULL;
    a->sub2 = NULL;
    a->t = NULL;
    a->e = NULL;
    a->scratch_re = NULL;
    a->scratch_im = NULL;
}

notorious_fft_aux* notorious_fft_mkaux_dft_1d(int N) {
    if (N <= 0) return NULL;

    notorious_fft_aux* a = (notorious_fft_aux*)malloc(sizeof(notorious_fft_aux));
    if (!a) return NULL;

    a->N = N;
    a->plan = notorious_fft_create_plan(N, 0);
    if (!a->plan) {
        free(a);
        return NULL;
    }

    notorious_fft_init_aux_from_plan(a);
    return a;
}

static notorious_fft_aux* notorious_fft_mkaux_dft_1d_internal(int N) {
    return notorious_fft_mkaux_dft_1d(N);
}

notorious_fft_aux* notorious_fft_mkaux_dft(int d, int* Ns) {
    return notorious_fft_make_aux(d, Ns, sizeof(notorious_fft_cmpl), notorious_fft_mkaux_dft_1d_internal);
}

notorious_fft_aux* notorious_fft_mkaux_dft_2d(int N1, int N2) {
    int Ns[2] = {N1, N2};
    return notorious_fft_mkaux_dft(2, Ns);
}

notorious_fft_aux* notorious_fft_mkaux_dft_3d(int N1, int N2, int N3) {
    int Ns[3] = {N1, N2, N3};
    return notorious_fft_mkaux_dft(3, Ns);
}

/* Precompute realdft twiddles and scratch buffers */
notorious_fft_aux* notorious_fft_mkaux_realdft_1d(int N) {
    if (N <= 0 || (N & (N - 1))) return NULL;

    notorious_fft_aux* a = (notorious_fft_aux*)malloc(sizeof(notorious_fft_aux));
    if (!a) return NULL;

    a->N = N;
    notorious_fft_init_aux_from_plan(a);

    if (N >= 4) {
        a->plan = notorious_fft_create_plan(N / 2, 0);
        if (!a->plan) {
            free(a);
            return NULL;
        }

        /* Precompute unpack twiddles: N/4 pairs (cos(-2πk/N), sin(-2πk/N)) */
        a->e = notorious_fft_malloc((size_t)(N / 2) * sizeof(notorious_fft_real));
        if (!a->e) {
            notorious_fft_destroy_plan(a->plan);
            free(a);
            return NULL;
        }
        {
            notorious_fft_real* tw = (notorious_fft_real*)a->e;
            for (int k = 0; k < N/4; k++) {
                notorious_fft_real angle = -NOTORIOUS_FFT_2PI * (notorious_fft_real)k / (notorious_fft_real)N;
                tw[2*k]   = notorious_fft_cos(angle);
                tw[2*k+1] = notorious_fft_sin(angle);
            }
        }

        /* Preallocate scratch: N reals (M interleaved complex for inner FFT) */
        a->t = notorious_fft_malloc((size_t)N * sizeof(notorious_fft_real));
        if (!a->t) {
            notorious_fft_free(a->e);
            notorious_fft_destroy_plan(a->plan);
            free(a);
            return NULL;
        }
    } else {
        a->plan = NULL;
    }

    return a;
}

static notorious_fft_aux* notorious_fft_mkaux_realdft_1d_internal(int N) {
    return notorious_fft_mkaux_realdft_1d(N);
}

notorious_fft_aux* notorious_fft_mkaux_realdft(int d, int* Ns) {
    if (d == 1) {
        return notorious_fft_mkaux_realdft_1d(Ns[0]);
    } else {
        int p = 1;
        for (int i = 0; i < d - 1; i++) p *= Ns[i];

        notorious_fft_aux* a = (notorious_fft_aux*)malloc(sizeof(notorious_fft_aux));
        if (!a) return NULL;

        a->N = Ns[d-1] * p;
        a->plan = NULL;
        a->sub1 = notorious_fft_mkaux_realdft_1d(Ns[d-1]);
        a->sub2 = notorious_fft_mkaux_dft(d - 1, Ns);

        if (!a->sub1 || !a->sub2) {
            notorious_fft_free_aux(a);
            return NULL;
        }

        return a;
    }
}

notorious_fft_aux* notorious_fft_mkaux_realdft_2d(int N1, int N2) {
    int Ns[2] = {N1, N2};
    return notorious_fft_mkaux_realdft(2, Ns);
}

notorious_fft_aux* notorious_fft_mkaux_realdft_3d(int N1, int N2, int N3) {
    int Ns[3] = {N1, N2, N3};
    return notorious_fft_mkaux_realdft(3, Ns);
}

/* DCT/DST Type 2/3: builds on realdft, adds precomputed DCT twiddles + scratch */
notorious_fft_aux* notorious_fft_mkaux_t2t3_1d(int N) {
    /* Start with realdft aux (gives us plan, unpack twiddles, realdft scratch) */
    notorious_fft_aux* a = notorious_fft_mkaux_realdft_1d(N);
    if (!a) return NULL;

    if (N >= 4) {
        /* Precompute DCT-2/3 post-processing twiddles:
         * N/2 pairs: (cos(-πk/(2N)), sin(-πk/(2N))) for k=0..N/2-1 */
        a->scratch_re = (notorious_fft_real*)notorious_fft_malloc((size_t)N * sizeof(notorious_fft_real));
        if (!a->scratch_re) {
            notorious_fft_free_aux(a);
            return NULL;
        }
        {
            notorious_fft_real* dct_tw = a->scratch_re;
            for (int k = 0; k < N/2; k++) {
                notorious_fft_real angle = -NOTORIOUS_FFT_PI * (notorious_fft_real)k / (notorious_fft_real)(2 * N);
                dct_tw[2*k]   = notorious_fft_cos(angle);
                dct_tw[2*k+1] = notorious_fft_sin(angle);
            }
        }

        /* Preallocate DCT scratch: reordered(N) + z_buf(N+2) */
        a->scratch_im = (notorious_fft_real*)notorious_fft_malloc((size_t)(2 * N + 2) * sizeof(notorious_fft_real));
        if (!a->scratch_im) {
            notorious_fft_free_aux(a);
            return NULL;
        }
    }

    return a;
}

static notorious_fft_aux* notorious_fft_mkaux_t2t3_1d_internal(int N) {
    return notorious_fft_mkaux_t2t3_1d(N);
}

notorious_fft_aux* notorious_fft_mkaux_t2t3(int d, int* Ns) {
    return notorious_fft_make_aux(d, Ns, sizeof(notorious_fft_real), notorious_fft_mkaux_t2t3_1d_internal);
}

notorious_fft_aux* notorious_fft_mkaux_t2t3_2d(int N1, int N2) {
    int Ns[2] = {N1, N2};
    return notorious_fft_mkaux_t2t3(2, Ns);
}

notorious_fft_aux* notorious_fft_mkaux_t2t3_3d(int N1, int N2, int N3) {
    int Ns[3] = {N1, N2, N3};
    return notorious_fft_mkaux_t2t3(3, Ns);
}

notorious_fft_aux* notorious_fft_mkaux_t4_1d(int N) {
    if (N <= 0 || (N & (N - 1))) return NULL;

    notorious_fft_aux* a = (notorious_fft_aux*)malloc(sizeof(notorious_fft_aux));
    if (!a) return NULL;

    a->N = N;
    a->plan = NULL;
    notorious_fft_init_aux_from_plan(a);

    if (N >= 2) {
        int M = N / 2;  /* inner DFT size */

        /* For N=2, M=1 — no plan needed, DFT is trivial identity */
        if (M >= 2) {
            a->plan = notorious_fft_create_plan(M, 0);
            if (!a->plan) { free(a); return NULL; }
        }

        /* e: premul twiddles exp(-iπn/N) for n=0..M-1  (M pairs = 2M reals)
         *    postmul twiddles exp(-iπ(2n+1)/(4N)) for n=0..N-1 (N pairs = 2N reals)
         * Total: 2*(M+N) reals = 2*(3N/2) = 3N reals */
        a->e = notorious_fft_malloc((size_t)(2 * M + 2 * N) * sizeof(notorious_fft_real));
        if (!a->e) {
            if (a->plan) notorious_fft_destroy_plan(a->plan);
            free(a);
            return NULL;
        }
        {
            notorious_fft_real* ep = (notorious_fft_real*)a->e;
            /* premul: exp(-iπn/N) for n=0..M-1 */
            for (int n = 0; n < M; n++) {
                notorious_fft_real ang = -NOTORIOUS_FFT_PI * (notorious_fft_real)n / (notorious_fft_real)N;
                ep[2*n]   = notorious_fft_cos(ang);
                ep[2*n+1] = notorious_fft_sin(ang);
            }
            /* postmul: exp(-iπ(2n+1)/(4N)) for n=0..N-1 */
            ep += 2 * M;
            for (int n = 0; n < N; n++) {
                notorious_fft_real ang = -NOTORIOUS_FFT_PI * (notorious_fft_real)(2*n + 1) / (notorious_fft_real)(4 * N);
                ep[2*n]   = notorious_fft_cos(ang);
                ep[2*n+1] = notorious_fft_sin(ang);
            }
        }

        /* t: scratch for M interleaved complex = 2M = N reals */
        a->t = notorious_fft_malloc((size_t)N * sizeof(notorious_fft_real));
        if (!a->t) {
            notorious_fft_free(a->e);
            if (a->plan) notorious_fft_destroy_plan(a->plan);
            free(a);
            return NULL;
        }
    }

    return a;
}

static notorious_fft_aux* notorious_fft_mkaux_t4_1d_internal(int N) {
    return notorious_fft_mkaux_t4_1d(N);
}

notorious_fft_aux* notorious_fft_mkaux_t4(int d, int* Ns) {
    return notorious_fft_make_aux(d, Ns, sizeof(notorious_fft_real), notorious_fft_mkaux_t4_1d_internal);
}

notorious_fft_aux* notorious_fft_mkaux_t4_2d(int N1, int N2) {
    int Ns[2] = {N1, N2};
    return notorious_fft_mkaux_t4(2, Ns);
}

notorious_fft_aux* notorious_fft_mkaux_t4_3d(int N1, int N2, int N3) {
    int Ns[3] = {N1, N2, N3};
    return notorious_fft_mkaux_t4(3, Ns);
}

static notorious_fft_aux* notorious_fft_make_aux(int d, int* Ns, int datasz,
                                    notorious_fft_aux* (*aux_1d)(int N)) {
    int p = 1;
    for (int i = 0; i < d; i++) p *= Ns[i];

    notorious_fft_aux* a = (notorious_fft_aux*)malloc(sizeof(notorious_fft_aux));
    if (!a) return NULL;

    a->N = p;
    a->plan = NULL;
    a->t = NULL;
    a->e = NULL;
    a->scratch_re = NULL;
    a->scratch_im = NULL;

    if (d == 1) {
        a->sub1 = NULL;
        a->sub2 = (*aux_1d)(Ns[0]);
    } else {
        a->t = notorious_fft_malloc((size_t)p * datasz);
        if (!a->t) {
            free(a);
            return NULL;
        }
        a->sub1 = notorious_fft_make_aux(d - 1, Ns + 1, datasz, aux_1d);
        a->sub2 = (*aux_1d)(Ns[0]);
    }

    if ((d > 1 && !a->sub1) || !a->sub2) {
        notorious_fft_free_aux(a);
        return NULL;
    }

    return a;
}

void notorious_fft_free_aux(notorious_fft_aux* a) {
    if (!a) return;
    if (a->plan) notorious_fft_destroy_plan(a->plan);
    notorious_fft_free(a->t);
    notorious_fft_free(a->e);
    notorious_fft_free(a->scratch_re);
    notorious_fft_free(a->scratch_im);
    notorious_fft_free_aux(a->sub1);
    notorious_fft_free_aux(a->sub2);
    free(a);
}

#endif /* NOTORIOUS_FFT_IMPLEMENTATION */

#ifdef __cplusplus
} /* extern "C" */
#endif

#endif /* NOTORIOUS_FFT_H */
